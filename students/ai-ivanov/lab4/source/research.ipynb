{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Список 20 новостных групп, как указано в описании датасета\n",
    "NEWSGROUP_NAMES = [\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.os.ms-windows.misc\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"comp.sys.mac.hardware\",\n",
    "    \"comp.windows.x\",\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"rec.motorcycles\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"rec.sport.hockey\",\n",
    "    \"sci.crypt\",\n",
    "    \"sci.electronics\",\n",
    "    \"sci.med\",\n",
    "    \"sci.space\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"talk.politics.guns\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"talk.religion.misc\",\n",
    "]  # Упорядочил для единообразия, если необходимо\n",
    "\n",
    "\n",
    "def load_documents_by_newsgroup(data_directory: str) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Загружает и группирует документы из датасета 20 Newsgroups.\n",
    "\n",
    "    Предполагается, что в data_directory находятся файлы для каждой из 20 новостных групп.\n",
    "    Имена файлов могут быть как 'alt.atheism.txt' или 'alt.atheism'.\n",
    "    Каждый файл содержит конкатенированные сообщения, где каждое сообщение\n",
    "    начинается с заголовков \"Newsgroup:\", \"Document_id:\", \"From:\", \"Subject:\",\n",
    "    за которыми следует тело сообщения.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_directory : str\n",
    "        Путь к директории, содержащей файлы новостных групп\n",
    "        (например, 'data/archive-7' или 'students/ai-ivanov/lab4/data/archive-7').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, list[str]]\n",
    "        Словарь, где ключи - это имена новостных групп (извлеченные из заголовков),\n",
    "        а значения - списки строк, содержащих тексты документов для этой группы.\n",
    "    \"\"\"\n",
    "    documents_by_group = defaultdict(list)\n",
    "\n",
    "    for newsgroup_filename_base in NEWSGROUP_NAMES:\n",
    "        # Сначала пробуем имя файла с .txt, как было упомянуто \"20 текстовых документов .txt\"\n",
    "        filepath_txt = os.path.join(data_directory, newsgroup_filename_base + \".txt\")\n",
    "        # Затем пробуем имя файла без расширения\n",
    "        filepath_no_ext = os.path.join(data_directory, newsgroup_filename_base)\n",
    "\n",
    "        actual_filepath = None\n",
    "        if os.path.exists(filepath_txt):\n",
    "            actual_filepath = filepath_txt\n",
    "        elif os.path.exists(filepath_no_ext):\n",
    "            actual_filepath = filepath_no_ext\n",
    "        else:\n",
    "            print(\n",
    "                f\"Предупреждение: Файл для группы '{newsgroup_filename_base}' не найден как '{filepath_txt}' или '{filepath_no_ext}'. Пропускается.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(actual_filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "            if not content.strip():  # Пропустить пустые файлы\n",
    "                continue\n",
    "\n",
    "            # Разделяем содержимое файла на отдельные сообщения.\n",
    "            # Сообщения разделяются строкой, начинающейся с \"Newsgroup:\", которой предшествует перевод строки.\n",
    "            # Используем re.MULTILINE для корректной работы ^\n",
    "            message_blocks = re.split(r\"\\n(?=^Newsgroup:)\", content, flags=re.MULTILINE)\n",
    "\n",
    "            for block_text in message_blocks:\n",
    "                block_text_stripped = block_text.strip()\n",
    "                if (\n",
    "                    not block_text_stripped\n",
    "                ):  # Пропустить пустые блоки (например, из-за ведущего \\n)\n",
    "                    continue\n",
    "\n",
    "                lines = block_text_stripped.split(\"\\n\")\n",
    "\n",
    "                parsed_newsgroup = None\n",
    "                subject_line_idx = -1\n",
    "\n",
    "                # Первой строкой блока должна быть \"Newsgroup: ...\"\n",
    "                if not lines[0].startswith(\"Newsgroup:\"):\n",
    "                    # Этот блок может быть \"мусором\" до первого настоящего заголовка Newsgroup:\n",
    "                    # print(f\"Пропускается блок, не начинающийся с 'Newsgroup:' в {actual_filepath}: '{lines[0][:70]}...'\")\n",
    "                    continue\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    if line.startswith(\"Newsgroup:\"):\n",
    "                        parsed_newsgroup = line.split(\":\", 1)[1].strip()\n",
    "                    elif line.startswith(\"Subject:\"):\n",
    "                        subject_line_idx = i\n",
    "                        break  # Заголовок Subject найден, основная часть заголовков обработана\n",
    "\n",
    "                if parsed_newsgroup and subject_line_idx != -1:\n",
    "                    # Тело документа - это все строки после строки Subject:\n",
    "                    # Убедимся, что есть строки после Subject\n",
    "                    if subject_line_idx < len(lines) - 1:\n",
    "                        body_lines = lines[subject_line_idx + 1 :]\n",
    "                        document_body = \"\\n\".join(body_lines).strip()\n",
    "\n",
    "                        if (\n",
    "                            document_body\n",
    "                        ):  # Добавляем только если тело документа не пустое\n",
    "                            documents_by_group[parsed_newsgroup].append(document_body)\n",
    "                    # else:\n",
    "                    # print(f\"Предупреждение: Блок с заголовком Subject, но без тела в {actual_filepath}. Newsgroup: {parsed_newsgroup}. Блок: {block_text_stripped[:100]}\")\n",
    "                # else:\n",
    "                # print(f\"Предупреждение: Не удалось распарсить блок или отсутствует Subject в {actual_filepath}. Newsgroup: {parsed_newsgroup}. Блок: {block_text_stripped[:100]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка при обработке файла {actual_filepath}: {e}\")\n",
    "\n",
    "    return dict(documents_by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_newsgroup = load_documents_by_newsgroup(\"../data/archive-7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_documents_by_newsgroup = {\n",
    "    group: documents_by_newsgroup[group][:20] for group in documents_by_newsgroup\n",
    "}\n",
    "\n",
    "documents_by_newsgroup = part_documents_by_newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 20/20 [00:03<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from preprocess import TextPreprocessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "preprocessor = TextPreprocessor(language=\"english\")\n",
    "\n",
    "processed_docs = preprocessor.preprocess_documents(\n",
    "    documents_by_newsgroup[\"alt.atheism\"]\n",
    ")\n",
    "\n",
    "\n",
    "for group in tqdm(documents_by_newsgroup, desc=\"Processing documents\"):\n",
    "    processed_docs = preprocessor.preprocess_documents(documents_by_newsgroup[group])\n",
    "\n",
    "    documents_by_newsgroup[group] = processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: 20\n",
      "comp.graphics: 20\n",
      "comp.os.ms-windows.misc: 20\n",
      "comp.sys.ibm.pc.hardware: 20\n",
      "comp.sys.mac.hardware: 20\n",
      "comp.windows.x: 20\n",
      "misc.forsale: 20\n",
      "rec.autos: 20\n",
      "rec.motorcycles: 20\n",
      "rec.sport.baseball: 20\n",
      "rec.sport.hockey: 20\n",
      "sci.crypt: 20\n",
      "sci.electronics: 20\n",
      "sci.med: 20\n",
      "sci.space: 20\n",
      "soc.religion.christian: 20\n",
      "talk.politics.guns: 20\n",
      "talk.politics.mideast: 20\n",
      "talk.politics.misc: 20\n",
      "talk.religion.misc: 20\n"
     ]
    }
   ],
   "source": [
    "for group in documents_by_newsgroup:\n",
    "    print(f\"{group}: {len(documents_by_newsgroup[group])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество документов для LDA: 400\n",
      "Инициализация LDA с 20 темами и 100 итерациями...\n",
      "Начало обучения LDA модели...\n",
      "\u001b[2m2025-05-16 17:13:22\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m0\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:13:31\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m5\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:13:39\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m10\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:13:47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m15\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:13:56\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m20\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:14:04\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m25\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:14:12\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m30\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:14:21\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m35\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:14:29\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m40\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:14:37\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m45\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:14:46\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m50\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:14:54\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m55\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:15:02\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m60\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:15:11\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m65\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:15:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m70\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:15:28\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m75\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:15:36\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m80\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:15:44\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m85\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:15:52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m90\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-05-16 17:16:01\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIteration                     \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m95\u001b[0m \u001b[36mn_iter\u001b[0m=\u001b[35m100\u001b[0m\n",
      "Обучение LDA модели завершено за 166.65 секунд.\n",
      "\n",
      "Топ-5 слов для первых 5 тем:\n",
      "Тема 1: ['space', 'technology', 'research', 'society', 'issue']\n",
      "Тема 2: ['space', 'mission', 'orbit', 'probe', 'launch']\n",
      "Тема 3: ['widget', 'use', 'resource', 'application', 'value']\n",
      "Тема 4: ['god', 'atheist', 'nt', 'religion', 'believe']\n",
      "Тема 5: ['period', 'pp', 'power', 'play', 'scorer']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from lda import LDA\n",
    "\n",
    "\n",
    "# 1. Подготовка данных: объединение всех документов в один список\n",
    "all_processed_documents = []\n",
    "for group_name in documents_by_newsgroup:\n",
    "    all_processed_documents.extend(documents_by_newsgroup[group_name])\n",
    "\n",
    "print(f\"Общее количество документов для LDA: {len(all_processed_documents)}\")\n",
    "\n",
    "# Проверка, что документы не пустые и содержат списки токенов\n",
    "if not all_processed_documents:\n",
    "    print(\"Нет документов для обучения LDA.\")\n",
    "elif not isinstance(all_processed_documents[0], list) or (\n",
    "    len(all_processed_documents[0]) > 0\n",
    "    and not isinstance(all_processed_documents[0][0], str)\n",
    "):\n",
    "    print(\n",
    "        \"Документы должны быть представлены как список списков токенов (строк). Пожалуйста, проверьте результаты предобработки.\"\n",
    "    )\n",
    "else:\n",
    "    # 2. Инициализация LDA модели\n",
    "    # Для примера: 20 тем, 100 итераций. Можно увеличить n_iter для лучшего качества.\n",
    "    # Alpha и Beta оставлены по умолчанию (0.1 и 0.01 соответственно)\n",
    "    # random_state для воспроизводимости\n",
    "    n_topics_lda = 20\n",
    "    n_iterations_lda = (\n",
    "        100  # Для быстрого теста, рекомендуется больше (например, 500-2000)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Инициализация LDA с {n_topics_lda} темами и {n_iterations_lda} итерациями...\"\n",
    "    )\n",
    "    # Убедимся, что lda.py на месте. Если он в том же каталоге, что и research.ipynb, то импорт 'from lda import LDA' должен сработать.\n",
    "    # Если lda.py в students/ai-ivanov/lab4/source/, а research.ipynb тоже там, то все ок.\n",
    "    # Если нет, нужно будет настроить sys.path или переместить файл.\n",
    "    # Предполагаем, что файл на месте.\n",
    "    lda_model = LDA(\n",
    "        n_topics=n_topics_lda,\n",
    "        n_iter=n_iterations_lda,\n",
    "        random_state=42,\n",
    "        alpha=0.1,\n",
    "        beta=0.1,\n",
    "    )\n",
    "\n",
    "    # 3. Обучение модели и замер времени\n",
    "    print(\"Начало обучения LDA модели...\")\n",
    "    start_time = time.time()\n",
    "    lda_model.fit(all_processed_documents)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Обучение LDA модели завершено за {training_time:.2f} секунд.\")\n",
    "\n",
    "    # 4. Вывод результатов (например, топ-5 слов для первых 5 тем)\n",
    "    print(\"\\nТоп-5 слов для первых 5 тем:\")\n",
    "    topics = lda_model.get_topics(top_n_words=5)\n",
    "    if topics:  # Проверка, что темы получены\n",
    "        for i, topic in enumerate(\n",
    "            topics[:5]\n",
    "        ):  # Показываем только первые 5 тем для краткости\n",
    "            topic_words = [word for word, prob in topic]\n",
    "            print(f\"Тема {i + 1}: {topic_words}\")\n",
    "    else:\n",
    "        print(\"Не удалось получить темы\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Тема   | Топ-5 слов                               |\n",
      "| :----- | :--------------------------------------- |\n",
      "| Тема 1 | `space`, `technology`, `research`, `society`, `issue` |\n",
      "| Тема 2 | `space`, `mission`, `orbit`, `probe`, `launch` |\n",
      "| Тема 3 | `widget`, `use`, `resource`, `application`, `value` |\n",
      "| Тема 4 | `god`, `atheist`, `nt`, `religion`, `believe` |\n",
      "| Тема 5 | `period`, `pp`, `power`, `play`, `scorer` |\n",
      "| Тема 6 | `drive`, `disk`, `system`, `hard`, `controller` |\n",
      "| Тема 7 | `rate`, `gun`, `homicide`, `handgun`, `vancouver` |\n",
      "| Тема 8 | `thanks`, `email`, `mouse`, `offer`, `call` |\n",
      "| Тема 9 | `use`, `driver`, `window`, `program`, `file` |\n",
      "| Тема 10 | `tax`, `court`, `mr`, `case`, `income` |\n",
      "| Тема 11 | `space`, `nasa`, `available`, `information`, `data` |\n",
      "| Тема 12 | `god`, `sin`, `say`, `christ`, `shall` |\n",
      "| Тема 13 | `entry`, `file`, `output`, `program`, `section` |\n",
      "| Тема 14 | `writes`, `article`, `kill`, `mother`, `henry` |\n",
      "| Тема 15 | `db`, `mov`, `bh`, `byte`, `si` |\n",
      "| Тема 16 | `game`, `team`, `win`, `run`, `player` |\n",
      "| Тема 17 | `nt`, `one`, `would`, `make`, `get` |\n",
      "| Тема 18 | `key`, `ripem`, `use`, `rsa`, `pgp` |\n",
      "| Тема 19 | `armenian`, `russian`, `people`, `turk`, `muslim` |\n",
      "| Тема 20 | `dod`, `ride`, `denizen`, `motorcycle`, `flame` |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = lda_model.get_topics(top_n_words=5)  # This line remains\n",
    "\n",
    "# Markdown table header\n",
    "md_output = \"| Тема   | Топ-5 слов                               |\\n\"\n",
    "md_output += \"| :----- | :--------------------------------------- |\\n\"\n",
    "\n",
    "for i, topic_data in enumerate(topics):\n",
    "    # topic_data is a list of (word, probability) tuples\n",
    "    topic_words = [word for word, prob in topic_data]\n",
    "    words_string = \", \".join(\n",
    "        [f\"`{word}`\" for word in topic_words]\n",
    "    )  # Format words as code\n",
    "    md_output += f\"| Тема {i + 1} | {words_string} |\\n\"\n",
    "\n",
    "print(md_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется 400 документов из переменной 'all_processed_documents'.\n",
      "Создание Document-Term Matrix с помощью CountVectorizer...\n",
      "Размерность Document-Term Matrix: (400, 6026)\n",
      "Инициализация scikit-learn LDA с 20 темами, alpha=0.1, beta=0.1, 20 EM итераций...\n",
      "Начало обучения LDA модели из scikit-learn...\n",
      "Обучение LDA модели из scikit-learn завершено за 0.77 секунд.\n",
      "\n",
      "Перплексия scikit-learn LDA модели: 1726.5928\n",
      "\n",
      "Топ-5 слов для каждой темы (scikit-learn LDA, Markdown формат):\n",
      "| Тема   | Топ-5 слов (scikit-learn)              |\\n| :----- | :--------------------------------------- |\\n| Тема 1 | `say`, `people`, `prophecy`, `armenian`, `dead` |\\n| Тема 2 | `writes`, `article`, `nt`, `right`, `titan` |\\n| Тема 3 | `mr`, `say`, `book`, `case`, `writes` |\\n| Тема 4 | `armenian`, `russian`, `turk`, `turkish`, `army` |\\n| Тема 5 | `db`, `probe`, `space`, `bh`, `mission` |\\n| Тема 6 | `key`, `ripem`, `use`, `period`, `rsa` |\\n| Тема 7 | `widget`, `use`, `application`, `resource`, `value` |\\n| Тема 8 | `entry`, `file`, `output`, `program`, `section` |\\n| Тема 9 | `god`, `nt`, `atheist`, `say`, `religion` |\\n| Тема 10 | `drive`, `disk`, `hard`, `controller`, `bios` |\\n| Тема 11 | `space`, `nasa`, `shuttle`, `mission`, `data` |\\n| Тема 12 | `run`, `game`, `hit`, `nt`, `writes` |\\n| Тема 13 | `space`, `use`, `driver`, `box`, `file` |\\n| Тема 14 | `nt`, `thanks`, `like`, `use`, `problem` |\\n| Тема 15 | `convenient`, `value`, `book`, `kmail`, `parallel` |\\n| Тема 16 | `writes`, `article`, `know`, `nt`, `like` |\\n| Тема 17 | `dod`, `motorcycle`, `ride`, `nt`, `know` |\\n| Тема 18 | `nt`, `people`, `make`, `right`, `writes` |\\n| Тема 19 | `game`, `article`, `team`, `run`, `writes` |\\n| Тема 20 | `gun`, `rate`, `homicide`, `handgun`, `vancouver` |\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# 0. Убедимся, что all_processed_documents существует и содержит списки токенов\n",
    "#    Эта переменная должна была быть создана в предыдущей ячейке при обучении кастомной LDA.\n",
    "if \"all_processed_documents\" not in globals() or not all_processed_documents:\n",
    "    print(\n",
    "        \"Переменная 'all_processed_documents' не найдена или пуста. \\n\"\n",
    "        \"Пожалуйста, убедитесь, что ячейка с обучением вашей LDA модели была выполнена \\n\"\n",
    "        \"и 'all_processed_documents' была корректно создана как список списков токенов.\"\n",
    "    )\n",
    "    # Примерное воссоздание, если нужно, но лучше выполнить предыдущую ячейку\n",
    "    # all_processed_documents = []\n",
    "    # for group_name in documents_by_newsgroup:\n",
    "    #     all_processed_documents.extend(documents_by_newsgroup[group_name])\n",
    "else:\n",
    "    print(\n",
    "        f\"Используется {len(all_processed_documents)} документов из переменной 'all_processed_documents'.\"\n",
    "    )\n",
    "\n",
    "    # 1. Подготовка данных для CountVectorizer:\n",
    "    # CountVectorizer ожидает список строк, поэтому объединяем токены каждого документа.\n",
    "    documents_as_strings = [\" \".join(doc) for doc in all_processed_documents]\n",
    "\n",
    "    # 2. Создание Document-Term Matrix\n",
    "    print(\"Создание Document-Term Matrix с помощью CountVectorizer...\")\n",
    "    # min_df=2: игнорировать термины, которые появляются менее чем в 2 документах\n",
    "    # max_df=0.95: игнорировать термины, которые появляются более чем в 95% документов (слишком частые)\n",
    "    vectorizer = CountVectorizer(min_df=2, max_df=0.95, stop_words=\"english\")\n",
    "    dtm = vectorizer.fit_transform(documents_as_strings)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"Размерность Document-Term Matrix: {dtm.shape}\")\n",
    "\n",
    "    # 3. Инициализация и обучение LDA модели из scikit-learn\n",
    "    n_topics_sklearn = 20\n",
    "    # Для 'batch' метода, max_iter - это EM итерации.\n",
    "    # 100 итераций Гиббса не равны 100 EM итерациям. Начнем с 10-20.\n",
    "    n_iterations_sklearn = 20\n",
    "\n",
    "    print(\n",
    "        f\"Инициализация scikit-learn LDA с {n_topics_sklearn} темами, alpha=0.1, beta=0.1, {n_iterations_sklearn} EM итераций...\"\n",
    "    )\n",
    "    lda_sklearn = LatentDirichletAllocation(\n",
    "        n_components=n_topics_sklearn,\n",
    "        doc_topic_prior=0.1,  # alpha\n",
    "        topic_word_prior=0.1,  # beta\n",
    "        learning_method=\"batch\",  # 'batch' или 'online'\n",
    "        max_iter=n_iterations_sklearn,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,  # Использовать все доступные CPU\n",
    "    )\n",
    "\n",
    "    print(\"Начало обучения LDA модели из scikit-learn...\")\n",
    "    start_time_sklearn = time.time()\n",
    "    lda_sklearn.fit(dtm)\n",
    "    end_time_sklearn = time.time()\n",
    "    training_time_sklearn = end_time_sklearn - start_time_sklearn\n",
    "    print(\n",
    "        f\"Обучение LDA модели из scikit-learn завершено за {training_time_sklearn:.2f} секунд.\"\n",
    "    )\n",
    "\n",
    "    # Добавляем вывод перплексии\n",
    "    print(f\"\\nПерплексия scikit-learn LDA модели: {lda_sklearn.perplexity(dtm):.4f}\")\n",
    "\n",
    "    # 4. Вывод резуль\n",
    "    # 4. Вывод результатов\n",
    "    print(\"\\nТоп-5 слов для каждой темы (scikit-learn LDA, Markdown формат):\")\n",
    "\n",
    "    md_output_sklearn_list = []\n",
    "    md_output_sklearn_list.append(\"| Тема   | Топ-5 слов (scikit-learn)              |\")\n",
    "    md_output_sklearn_list.append(\n",
    "        \"| :----- | :--------------------------------------- |\"\n",
    "    )\n",
    "\n",
    "    for topic_idx, topic_probs in enumerate(lda_sklearn.components_):\n",
    "        # lda_sklearn.components_ это topic-word distribution (не нормализованная)\n",
    "        # Берем индексы топ-N слов для текущей темы\n",
    "        top_n_words_indices = topic_probs.argsort()[: -5 - 1 : -1]\n",
    "        topic_words = [feature_names[i] for i in top_n_words_indices]\n",
    "        words_string = \", \".join([f\"`{word}`\" for word in topic_words])\n",
    "        md_output_sklearn_list.append(f\"| Тема {topic_idx + 1} | {words_string} |\")\n",
    "\n",
    "    print(\"\\\\n\".join(md_output_sklearn_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет перплексии на тестовом наборе данных...\n",
      "Размер тестового набора для перплексии (из all_processed_documents): 80 документов\n",
      "\u001b[2m2025-05-16 17:27:17\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mTransform iteration           \u001b[0m \u001b[36miteration\u001b[0m=\u001b[35m0\u001b[0m \u001b[36mn_transform_iter\u001b[0m=\u001b[35m5\u001b[0m\n",
      "Перплексия модели LDA на тестовом наборе: 1456.1181\n"
     ]
    }
   ],
   "source": [
    "# Расчет перплексии для оценки качества модели LDA\n",
    "# Используем тестовый набор данных.\n",
    "\n",
    "# В lda.py пример test_docs_for_perplexity был:\n",
    "# test_docs_for_perplexity = [\n",
    "#     [\"sweet\", \"fruit\", \"recipe\", \"healthy\", \"banana\"],\n",
    "#     [\"code\", \"algorithm\", \"software\", \"system\", \"computer\", \"science\"],\n",
    "#     [\"food\", \"diet\", \"apple\", \"vegetable\"],\n",
    "#     [\"unknown\", \"words\", \"only\"], # Этот документ будет иметь 0 известных слов\n",
    "#     [] # Пустой документ\n",
    "# ]\n",
    "# Для более корректной оценки, мы разделим all_processed_documents, если они доступны.\n",
    "\n",
    "print(\"Расчет перплексии на тестовом наборе данных...\")\n",
    "\n",
    "# Предполагаем, что all_processed_documents - это все доступные данные.\n",
    "# Разделим их на обучающий и тестовый наборы для более корректной оценки.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_docs_for_perplexity_calc = []\n",
    "\n",
    "if (\n",
    "    \"all_processed_documents\" in globals()\n",
    "    and isinstance(all_processed_documents, list)\n",
    "    and len(all_processed_documents) > 10\n",
    "):  # Нужен достаточный размер для разделения\n",
    "    # Проверяем, что все элементы в all_processed_documents являются списками (документами)\n",
    "    if all(isinstance(doc, list) for doc in all_processed_documents):\n",
    "        train_docs, test_docs_for_perplexity_calc = train_test_split(\n",
    "            all_processed_documents, test_size=0.2, random_state=42\n",
    "        )\n",
    "        print(\n",
    "            f\"Размер тестового набора для перплексии (из all_processed_documents): {len(test_docs_for_perplexity_calc)} документов\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Предупреждение: 'all_processed_documents' содержит элементы, не являющиеся списками. Используется демонстрационный набор.\"\n",
    "        )\n",
    "        test_docs_for_perplexity_calc = []  # Сбрасываем, чтобы использовать демонстрационный набор\n",
    "else:\n",
    "    print(\n",
    "        \"Недостаточно данных в 'all_processed_documents' или переменная не определена/некорректна.\"\n",
    "    )\n",
    "    print(\"Перплексия будет рассчитана на демонстрационном наборе данных.\")\n",
    "\n",
    "if (\n",
    "    not test_docs_for_perplexity_calc\n",
    "):  # Если разделение не удалось или не было выполнено\n",
    "    print(\"Используется демонстрационный тестовый набор для расчета перплексии.\")\n",
    "    test_docs_for_perplexity_calc = [\n",
    "        [\"sweet\", \"fruit\", \"recipe\", \"healthy\", \"banana\"],\n",
    "        [\"code\", \"algorithm\", \"software\", \"system\", \"computer\", \"science\"],\n",
    "        [\"food\", \"diet\", \"apple\", \"vegetable\"],\n",
    "        [\"video\", \"game\", \"play\", \"online\", \"software\", \"computer\"],\n",
    "        [\"space\", \"mission\", \"nasa\", \"orbit\", \"launch\"],\n",
    "        [\"research\", \"science\", \"study\", \"university\"],\n",
    "    ]\n",
    "    # Убедимся, что в демонстрационном наборе есть слова, которые модель могла видеть,\n",
    "    # иначе перплексия может быть очень высокой или не рассчитаться.\n",
    "    # В идеале, этот набор должен быть репрезентативным.\n",
    "\n",
    "if (\n",
    "    not test_docs_for_perplexity_calc\n",
    "):  # Если все еще пуст (маловероятно здесь, но на всякий случай)\n",
    "    print(\"Тестовые данные для перплексии пусты. Расчет невозможен.\")\n",
    "else:\n",
    "    # Вы можете передать n_transform_iter_override, если хотите другое количество итераций для transform внутри perplexity\n",
    "    # perplexity_score = lda_model.perplexity(test_docs_for_perplexity_calc, n_transform_iter_override=20)\n",
    "    perplexity_score = lda_model.perplexity(train_docs)\n",
    "\n",
    "    if perplexity_score is not None:\n",
    "        print(f\"Перплексия модели LDA на тестовом наборе: {perplexity_score:.4f}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Не удалось рассчитать перплексию. Убедитесь, что модель обучена и тестовые данные корректны (содержат известные словарю слова).\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
