# **Отчет по лабораторной работе №4: Латентное размещение Дирихле**

## **Описание алгоритма латентного размещения Дирихле (LDA)**

Латентное размещение Дирихле (Latent Dirichlet Allocation, LDA) — это вероятностная модель тематического моделирования, позволяющая выявлять скрытые темы в коллекции текстов. В модели предполагается, что:

* каждый документ представляет собой смесь тем;
* каждая тема задается распределением вероятностей по словам.

В данной работе реализована версия LDA с использованием **сэмплирования Гиббса**, которое является методом стохастической оптимизации и итеративного уточнения скрытых переменных.

Основные параметры модели:

* Количество тем (topics): фиксируется заранее;
* Параметры априорных распределений: α (документ-тема) и β (тема-слово);
* Количество итераций: количество прогонов алгоритма.

## **Описание датасета**

В качестве текстового датасета использован стандартный набор **20 Newsgroups**, предоставляемый библиотекой `sklearn.datasets`.

### **Краткое описание:**

* Количество документов: 1000 (взята подвыборка из тестовой части коллекции);
* Тип данных: короткие текстовые сообщения из интернет-форумов;
* Задача: тематическое моделирование для выявления скрытых тем;
* Предобработка:

  * Удалены стоп-слова;
  * Ограничен словарь 1000 наиболее частотными словами.

## **Результаты экспериментов**

### **Собственная реализация (GibbsLDA)**

* Количество тем: 5
* Количество итераций: 30
* Время обучения: **4.34 секунд**
* Перплексия на тестовой выборке: **1011.24**

Примеры наиболее характерных слов по темам:

| Тема | Топ-10 слов                                                               |
| ---- | ------------------------------------------------------------------------- |
| 1    | god, people, don, think, say, know, jesus, good, bible, does              |
| 2    | people, know, like, don, good, think, does, time, say, way                |
| 3    | car, engine, new, power, speed, cars, bike, time, like, don               |
| 4    | window, use, file, windows, program, dos, problem, using, software, drive |
| 5    | gun, people, weapons, fire, control, guns, don, know, like, used          |

### **Эталонная реализация (LatentDirichletAllocation из sklearn)**

* Количество тем: 5
* Количество итераций: 30
* Время обучения: **0.72 секунд**
* Перплексия на тестовой выборке: **918.65**

## **Сравнение с эталонной реализацией**

| Модель                 | Время обучения (с) | Перплексия |
| ---------------------- | ------------------ | ---------- |
| Собственная (GibbsLDA) | 4.34               | 1011.24    |
| Sklearn LDA            | 0.72               | 918.65     |

## **Выводы**

* Собственная реализация алгоритма LDA с использованием сэмплирования Гиббса корректно работает и позволяет выявлять тематику документов.
* Эталонная реализация из библиотеки `sklearn` показывает более высокое качество (меньшую перплексию) и существенно быстрее обучается.
* Отличие в производительности связано с тем, что библиотечная версия использует более оптимизированные численные методы и реализована на низкоуровневом уровне.
* Тем не менее, собственная реализация позволяет детально изучить внутренние механизмы работы тематического моделирования и принципы сэмплирования Гиббса.
