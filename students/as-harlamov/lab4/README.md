# Лабораторная работа №4. Латентное размещение Дирихле

В рамках данной лабораторной работы предстоит реализовать алгоритм латентного размещения Дирихле (LDA) и сравнить его с эталонной реализацией из библиотеки `scikit-learn`.

## 1. Описание алгоритма LDA

**Latent Dirichlet Allocation (LDA)** — это вероятностная модель, используемая для тематического моделирования коллекций текстовых документов. Модель предполагает, что:

* Каждый документ представляет собой смесь тем.
* Каждая тема — это распределение над словарём слов.
* Генерация слов в документах происходит случайно на основе этих распределений.

**Основные шаги алгоритма:**

* Инициализация случайных тем для каждого слова.
* Повторный проход по словам и перераспределение тем на основе условных вероятностей.
* После нескольких итераций получается устойчивое распределение тем по документам и слов по темам.

Для реализации использовался метод **Gibbs Sampling**.

## 2. Датасет

**Использованный датасет:**

`fetch_20newsgroups` (подмножество: спорт, медицина, политика, компьютеры)

**Характеристики:**

* Количество документов: ~2500
* Тематика: спортивные новости, медицинские статьи, политика, технические обсуждения
* Язык: английский

**Предобработка:**

* Токенизация с помощью `nltk.word_tokenize`
* Удаление стоп-слов (`nltk.corpus.stopwords`)
* Фильтрация коротких слов
* Удаление пустых документов

## 3. Реализация LDA

**Особенности реализации:**

* Использование метода Gibbs Sampling
* Ручное управление частотами слов и тем
* Поддержка основных гиперпараметров: `alpha`, `beta`, `n_iter`

**Ключевые этапы:**

* Создание корпуса и словаря
* Инициализация матриц частот: `doc_topic_counts`, `topic_word_counts`
* Итеративное обновление тем для каждого слова
* Получение топ-слов по темам

**Пример вывода тем:**

```
Тема 0: ['baseball', 'game', 'team', 'hit', 'runs']
Тема 1: ['israel', 'palestinian', 'government', 'war']
Тема 2: ['patient', 'disease', 'health', 'treatment']
```

### Как работает Gibbs Sampling в LDA

**Gibbs Sampling** — это алгоритм из семейства методов Монте-Карло по схеме цепей Маркова, используемый для генерации выборки из многомерного распределения вероятностей, когда прямая выборка затруднена.

В контексте LDA Gibbs Sampling используется для оценки скрытых тем в документах и распределений слов по темам.

1. Каждому слову в каждом документе случайно назначается тема.
2. Для каждого слова:
   * Удаляется текущая тема из счётчиков.
   * Вычисляется вероятность принадлежности слова к каждой теме на основе:
     * частоты этой темы в документе
     * частоты слова в этой теме 
   * Назначается новая тема согласно вычисленной вероятности.
3. Процесс повторяется много раз (итераций), пока модель не "сойдётся".

## 4. Эталонная реализация (sklearn)

**Модель:**

```python
from sklearn.decomposition import LatentDirichletAllocation
```

**Подготовка данных:**

* Векторизация с помощью `CountVectorizer`
* Обучение LDA на матрице TF (term frequency)

## 5. Сравнение и оценка качества

| Реализация | Время обучения | Когерентность |
|------------|----------------|---------------|
| Custom     | 59.18          | 0.50          |
| Sklearn    | 7.50           | 0.59          |

`CoherenceModel` — это инструмент библиотеки `gensim`, который позволяет оценить качество тематической модели, то есть насколько хорошо выделенные темы соответствуют человеческой интерпретации текста.

Он вычисляет **когерентность (coherence)** тем — метрику, которая показывает, **насколько часто слова одной темы встречаются вместе в одних и тех же документах**.

Типы когерентности в `CoherenceModel`:

* `u_mass` (UMass coherence) – самая простая и быстрая. Считает лог-шанс появления пар слов в одних и тех же документах. Не требует корпуса или словаря.
* `c_v` (CV coherence) – наиболее интерпретируемая. Использует 10-граммовую TF-IDF матрицу и находит корреляцию слов через cosine similarity. Требует дополнительных данных.
* `c_npmi` (Normalized Pointwise Mutual Information) – улучшенный вариант PMI с нормализацией. Хорошо работает с большими корпусами.
* `c_uci` (UCI coherence) – основана на PMI между парами слов, но без нормализации. Часто используется в исследованиях.
* `c_we` (Word Embedding-based coherence) – использует предобученные эмбеддинги (например, Word2Vec, GloVe) для оценки схожести слов.

## Задание

1. Выбрать текстовый датасет для анализа, например, на [kaggle](https://www.kaggle.com/datasets).
2. Реализовать алгоритм латентного размещения Дирихле.
3. Обучить модель на выбранном датасете.
4. Оценить качество модели с использованием метрик когерентности тем.
5. Замерить время обучения модели.
6. Сравнить результаты с эталонной реализацией из библиотеки [scikit-learn](https://scikit-learn.org/stable/):
   * когерентность тем;
   * время обучения.
7. Подготовить отчет, включающий:
   * описание алгоритма латентного размещения Дирихле;
   * описание датасета;
   * результаты экспериментов;
   * сравнение с эталонной реализацией;
   * выводы.

