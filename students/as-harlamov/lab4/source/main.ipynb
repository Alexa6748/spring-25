{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T07:42:47.021312Z",
     "start_time": "2025-05-22T07:42:46.289357Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/wignorbo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wignorbo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Загрузка и предобработка данных"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "984ba4610b001846"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    remove=('headers', 'footers', 'quotes'), \n",
    "    categories=['rec.sport.baseball', 'comp.sys.mac.hardware', 'sci.med', 'talk.politics.mideast'],\n",
    ")\n",
    "documents = data.data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T07:42:50.697029Z",
     "start_time": "2025-05-22T07:42:49.999763Z"
    }
   },
   "id": "efd800522f48fbb5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        tokens = word_tokenize(re.sub(r'\\W+', ' ', doc.lower()))\n",
    "        filtered = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "        processed.append(filtered)\n",
    "    return processed\n",
    "\n",
    "processed_docs = preprocess(documents)\n",
    "processed_docs = [doc for doc in processed_docs if len(doc) > 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T07:44:34.103258Z",
     "start_time": "2025-05-22T07:44:33.165111Z"
    }
   },
   "id": "b1bf08155c6bca40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Построение словаря и корпуса"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aac3b742ea9ef8f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "dictionary = defaultdict()\n",
    "dictionary.default_factory = lambda: len(dictionary)\n",
    "vocab_size = 0\n",
    "\n",
    "corpus = []\n",
    "for doc in processed_docs:\n",
    "    bow = []\n",
    "    for word in doc:\n",
    "        idx = dictionary[word]\n",
    "        bow.append(idx)\n",
    "    corpus.append(bow)\n",
    "    vocab_size = max(vocab_size, max(bow) + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T07:44:37.124581Z",
     "start_time": "2025-05-22T07:44:37.106302Z"
    }
   },
   "id": "3f0afd91948a0b8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Обучение модели"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37dae731038901a6"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обучения (custom): 59.181934118270874\n",
      "Тема 0: two, well, think, day, government, right, armenia, year, three, anything\n",
      "Тема 1: jews, israeli, also, jewish, state, greek, human, food, water, rights\n",
      "Тема 2: health, medical, research, 1993, information, new, university, number, disease, center\n",
      "Тема 3: people, could, see, years, say, come, think, way, even, came\n",
      "Тема 4: mac, com, keyboard, scsi, software, used, memory, hardware, monitor, disk\n",
      "Тема 5: armenian, armenians, one, turkish, said, killed, children, went, first, anti\n",
      "Тема 6: israel, people, would, genocide, turks, world, right, could, please, give\n",
      "Тема 7: one, know, problem, use, apple, better, system, still, team, last\n",
      "Тема 8: would, get, like, time, also, back, something, good, going, much\n",
      "Тема 9: edu, may, turkey, know, game, soon, cancer, win, first, san\n"
     ]
    }
   ],
   "source": [
    "from lda import LDA\n",
    "\n",
    "\n",
    "index_to_word = {v: k for k, v in dictionary.items()}\n",
    "\n",
    "lda_custom = LDA(n_topics=10, n_iter=20)\n",
    "start_time = time.time()\n",
    "lda_custom.fit(corpus, vocab_size)\n",
    "end_time = time.time()\n",
    "\n",
    "training_time_custom = end_time - start_time\n",
    "print(\"Время обучения (custom):\", training_time_custom)\n",
    "\n",
    "topics_custom = lda_custom.get_vocabulary(index_to_word)\n",
    "for i, words in enumerate(topics_custom):\n",
    "    print(f\"Тема {i}: {', '.join(words)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T07:45:58.142299Z",
     "start_time": "2025-05-22T07:44:58.947997Z"
    }
   },
   "id": "3e2cedc2b1aaf6df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Оценка когерентности тем"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3803c387b55d3dc"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когерентность (custom): 0.4995167082720335\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "gensim_dictionary = Dictionary(processed_docs)\n",
    "\n",
    "coherence_model_custom = CoherenceModel(\n",
    "    topics=topics_custom, \n",
    "    texts=processed_docs, \n",
    "    dictionary=gensim_dictionary, \n",
    "    coherence='c_v',\n",
    ")\n",
    "coherence_custom = coherence_model_custom.get_coherence()\n",
    "print(\"Когерентность (custom):\", coherence_custom)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T07:47:15.266939Z",
     "start_time": "2025-05-22T07:47:11.089350Z"
    }
   },
   "id": "6652bb562446ba37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Сравнение с sklearn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b2129e67e47b213"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обучения (sklearn): 7.499375104904175\n",
      "Когерентность (sklearn): 0.5740620318969729\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = vectorizer.fit_transform(documents)\n",
    "\n",
    "start_time_sk = time.time()\n",
    "lda_sklearn = LatentDirichletAllocation(n_components=10, random_state=42, max_iter=20)\n",
    "lda_sklearn.fit(tf)\n",
    "end_time_sk = time.time()\n",
    "\n",
    "training_time_sk = end_time_sk - start_time_sk\n",
    "print(\"Время обучения (sklearn):\", training_time_sk)\n",
    "\n",
    "def get_sklearn_topics(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(top_words)\n",
    "    return topics\n",
    "\n",
    "topics_sklearn = get_sklearn_topics(lda_sklearn, vectorizer.get_feature_names_out(), 10)\n",
    "\n",
    "dictionary_sk = corpora.Dictionary([vectorizer.get_feature_names_out().tolist()]) \n",
    "\n",
    "cm_sklearn = CoherenceModel(\n",
    "    topics=[\n",
    "        [dictionary_sk.token2id[w] for w in topic] \n",
    "        for topic in topics_sklearn\n",
    "    ],\n",
    "    texts=processed_docs,\n",
    "    dictionary=gensim_dictionary,\n",
    "    coherence='c_v',\n",
    ")\n",
    "coherence_sklearn = cm_sklearn.get_coherence()\n",
    "print(\"Когерентность (sklearn):\", coherence_sklearn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T07:54:54.096693Z",
     "start_time": "2025-05-22T07:54:42.557091Z"
    }
   },
   "id": "63f999737ada595"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема 0: edu, gordon, banks, soon, geb, pitt, intellect, don, skepticism, n3jxp\n",
      "Тема 1: mac, apple, bit, card, scsi, problem, 32, use, monitor, color\n",
      "Тема 2: year, game, team, 00, good, games, runs, players, hit, better\n",
      "Тема 3: msg, don, water, food, like, just, know, think, adam, people\n",
      "Тема 4: drive, know, just, thanks, like, does, ve, mail, apple, good\n",
      "Тема 5: israel, israeli, just, think, arab, people, like, know, don, time\n",
      "Тема 6: said, people, don, know, didn, went, say, just, came, told\n",
      "Тема 7: edu, com, university, people, medical, cancer, patients, pain, disease, hiv\n",
      "Тема 8: armenian, turkish, armenians, jews, people, turkey, government, turks, armenia, greek\n",
      "Тема 9: health, use, keyboard, 1993, information, 10, number, medical, 20, edu\n"
     ]
    }
   ],
   "source": [
    "for i, words in enumerate(topics_sklearn):\n",
    "    print(f\"Тема {i}: {', '.join(words)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T07:56:23.981059Z",
     "start_time": "2025-05-22T07:56:23.942631Z"
    }
   },
   "id": "a60befc8227ca56c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "66f64f408324be36"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
