
# Лабораторная работа №4. Латентное размещение Дирихле (LDA)

## 1. Описание датасета

**Название**: 20 Newsgroups (subset)  
**Источник**: scikit-learn  
**Количество документов**: 100  
**Количество категорий**: 4  
**Категории**: sci.space, comp.graphics, rec.sport.baseball, talk.politics.mideast  

## 2. Описание алгоритма LDA

Латентное размещение Дирихле (LDA) - вероятностная тематическая модель, используемая для обнаружения скрытых тематик в коллекции документов.

### Основные предположения:
1. Каждый документ представляет собой смесь нескольких тем
2. Каждая тема представляет собой распределение над словами

### Процесс генерации документов:
1. Для каждого документа:
   a. Выбирается распределение тем θ ~ Dir(α)
2. Для каждого слова в документе:
   a. Выбирается тема z ~ Multinomial(θ)
   b. Выбирается слово w из p(w|z, β) - распределения слов темы z

### Обучение модели:
- Реализован метод сэмплирования по Гиббсу
- Альтернативный подход (в sklearn): вариационный Байес

### Особенности реализации:
- Сэмплирование по Гиббсу для оценки параметров
- Гиперпараметры α и β сглаживают распределения
- Итеративное обновление назначений тем

## 3. Результаты экспериментов

### Параметры моделей:
- Количество тем: 4
- α = 0.1, β = 0.01
- Количество итераций: 100 (собственная), 10 (sklearn)

### Метрики качества:
| Модель         | Когерентность (c_v) | Время обучения (сек) |
|----------------|----------------------|----------------------|
| Собственный LDA | 0.5620 | 75.32 |
| Sklearn LDA | 0.5563 | 0.58 |

### Визуализация тем:
![Топ-слова тем](https://github.com/user-attachments/assets/d0a32867-df69-4f2d-9d73-af2a77bec076)

## 4. Сравнение с эталонной реализацией

**Когерентность тем**:
- Собственная реализация: 0.5620
- Эталонная реализация: 0.5563
- Разница в когерентности: -0.0057

**Время обучения**:
- Собственная реализация: 75.32 сек
- Эталонная реализация: 0.58 сек
- Отношение времени: 129.96x

**Основные отличия**:
1. Эталонная реализация использует вариационный Байес вместо сэмплирования по Гиббсу
2. Sklearn реализация оптимизирована для производительности
3. В эталонной реализации используются векторизованные операции
4. Реализация sklearn поддерживает онлайн-обучение

## 5. Выводы

1. LDA эффективно выявляет скрытые темы в документах
2. Когерентность тем - основная метрика оценки качества тематической модели
3. Эталонная реализация значительно быстрее благодаря оптимизациям
4. Собственная реализация (сэмплирование по Гиббсу) может давать более точные результаты при большем числе итераций
5. Преимущества LDA:
   - Интерпретируемость тем
   - Возможность работы с большими коллекциями документов
   - Обнаружение скрытых структур

Ограничения:
- Требует предварительного задания количества тем
- Чувствительность к гиперпараметрам
- Вычислительная сложность

Рекомендации:
- Использовать GridSearch для подбора оптимального количества тем
- Экспериментировать с различными методами предобработки текста
- Применять тематические модели для:
  * Классификации документов
  * Поиска похожих документов
  * Визуализации больших текстовых коллекций
