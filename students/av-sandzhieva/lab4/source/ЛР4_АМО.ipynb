{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac52c853-666a-431c-a0e1-60bf41596b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d9a35d-8398-4abb-8085-fc0da2a40865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка необходимых ресурсов NLTK\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e2c7c2-d8c6-4c38-a052-a68c7448f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLDA:\n",
    "    def __init__(self, n_topics=10, alpha=0.1, beta=0.01, n_iter=1000, random_state=None):\n",
    "        self.n_topics = n_topics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.components_ = None\n",
    "        self.topic_word_ = None\n",
    "        self.doc_topic_ = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Инициализация параметров\n",
    "        n_docs, n_words = X.shape\n",
    "        random_state = np.random.RandomState(self.random_state)\n",
    "        \n",
    "        # Инициализация матриц подсчетов\n",
    "        n_dk = np.zeros((n_docs, self.n_topics)) + self.alpha  # счетчик, сколько раз тема k встречается в документе d (плюс сглаживание alpha).\n",
    "        n_kw = np.zeros((self.n_topics, n_words)) + self.beta   # счетчик, сколько раз слово w встречается в теме k (плюс сглаживание beta).\n",
    "        n_k = np.zeros(self.n_topics) + n_words * self.beta     # сумма слов в теме k (плюс beta * n_words).\n",
    "        \n",
    "        # Инициализация тематических назначений\n",
    "        z = []\n",
    "        doc_word_indices = []\n",
    "        \n",
    "        # Преобразование разреженной матрицы в формат, удобный для обработки\n",
    "        X_dense = X.toarray()\n",
    "        \n",
    "        for d in range(n_docs):\n",
    "            doc_z = []\n",
    "            doc_words = []\n",
    "            for w in range(n_words):\n",
    "                count = int(X_dense[d, w])\n",
    "                if count > 0:\n",
    "                    # Начальное случайное назначение темы\n",
    "                    topics = random_state.choice(self.n_topics, count, replace=True)\n",
    "                    doc_z.extend(topics)\n",
    "                    doc_words.extend([w] * count)\n",
    "                    \n",
    "                    # Обновление счетчиков\n",
    "                    for t in topics:\n",
    "                        n_dk[d, t] += 1\n",
    "                        n_kw[t, w] += 1\n",
    "                        n_k[t] += 1\n",
    "            z.append(np.array(doc_z))\n",
    "            doc_word_indices.append(np.array(doc_words))\n",
    "        \n",
    "        # Сэмплирование по Гиббсу\n",
    "        for iteration in tqdm(range(self.n_iter), desc=\"Gibbs Sampling\"):\n",
    "            for d in range(n_docs):\n",
    "                word_indices = doc_word_indices[d]\n",
    "                doc_len = len(z[d])\n",
    "                \n",
    "                for i in range(doc_len):\n",
    "                    w = word_indices[i]\n",
    "                    t = z[d][i]\n",
    "                    \n",
    "                    # Удаление текущего назначения\n",
    "                    n_dk[d, t] -= 1\n",
    "                    n_kw[t, w] -= 1\n",
    "                    n_k[t] -= 1\n",
    "                    \n",
    "                    # Расчет вероятностей для нового назначения\n",
    "                    p_topic = (n_dk[d, :] + self.alpha) * (n_kw[:, w] + self.beta) / (n_k + n_words * self.beta)\n",
    "                    \n",
    "                    # Нормализация вероятностей\n",
    "                    p_topic_sum = p_topic.sum()\n",
    "                    if p_topic_sum > 0:\n",
    "                        p_topic /= p_topic_sum\n",
    "                    else:\n",
    "                        p_topic = np.ones(self.n_topics) / self.n_topics\n",
    "                    \n",
    "                    # Выбор новой темы\n",
    "                    t = random_state.choice(self.n_topics, p=p_topic)\n",
    "                    z[d][i] = t\n",
    "                    \n",
    "                    # Обновление счетчиков\n",
    "                    n_dk[d, t] += 1\n",
    "                    n_kw[t, w] += 1\n",
    "                    n_k[t] += 1\n",
    "        \n",
    "        # Расчет итоговых распределений\n",
    "        self.components_ = (n_kw + self.beta) / (n_k[:, np.newaxis] + n_words * self.beta)\n",
    "        self.topic_word_ = self.components_\n",
    "        self.doc_topic_ = (n_dk + self.alpha) / (n_dk.sum(axis=1)[:, np.newaxis] + self.n_topics * self.alpha)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        n_docs, n_words = X.shape\n",
    "        random_state = np.random.RandomState(self.random_state)\n",
    "    \n",
    "    \n",
    "        n_dk = np.zeros((n_docs, self.n_topics)) + self.alpha\n",
    "        n_kw = np.zeros((self.n_topics, n_words)) + self.beta\n",
    "        n_k = np.zeros(self.n_topics) + n_words * self.beta\n",
    "\n",
    "        # Преобразование разреженной матрицы в плотную\n",
    "        X_dense = X.toarray()\n",
    "    \n",
    "        # Инициализация тематических назначений\n",
    "        z = []\n",
    "        doc_word_indices = []\n",
    "        for d in range(n_docs):\n",
    "            doc_z = []\n",
    "            doc_words = []\n",
    "            for w in range(n_words):\n",
    "                count = int(X_dense[d, w])\n",
    "                if count > 0:\n",
    "                    topics = random_state.choice(self.n_topics, count, replace=True)\n",
    "                    doc_z.extend(topics)\n",
    "                    doc_words.extend([w] * count)\n",
    "                    for t in topics:\n",
    "                        n_dk[d, t] += 1\n",
    "                        n_kw[t, w] += 1\n",
    "                        n_k[t] += 1\n",
    "            z.append(np.array(doc_z))\n",
    "            doc_word_indices.append(np.array(doc_words))\n",
    "\n",
    "        # Гиббсовское сэмплирование для новых документов\n",
    "        for iteration in range(self.n_iter // 10):  # Меньше итераций для ускорения\n",
    "            for d in range(n_docs):\n",
    "                word_indices = doc_word_indices[d]\n",
    "                doc_len = len(z[d])\n",
    "                for i in range(doc_len):\n",
    "                    w = word_indices[i]\n",
    "                    t = z[d][i]\n",
    "\n",
    "                    # Удаление текущего назначения\n",
    "                    n_dk[d, t] -= 1\n",
    "                    n_kw[t, w] -= 1\n",
    "                    n_k[t] -= 1\n",
    "\n",
    "                    # Расчёт вероятностей\n",
    "                    p_topic = (n_dk[d, :] + self.alpha) * (n_kw[:, w] + self.beta) / (n_k + n_words * self.beta)\n",
    "                    p_topic /= p_topic.sum()\n",
    "\n",
    "                    # Выбор новой темы\n",
    "                    t = random_state.choice(self.n_topics, p=p_topic)\n",
    "                    z[d][i] = t\n",
    "\n",
    "                    # Обновление счетчиков\n",
    "                    n_dk[d, t] += 1\n",
    "                    n_kw[t, w] += 1\n",
    "                    n_k[t] += 1\n",
    "\n",
    "        return n_dk / n_dk.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    def get_top_words(self, feature_names, n_top_words=10):\n",
    "        top_words = []\n",
    "        for topic_idx, topic in enumerate(self.components_):\n",
    "            top_words_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "            top_words.append([feature_names[i] for i in top_words_indices])\n",
    "        return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428d1b5d-6553-45b2-a355-5c0904e9c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Удаление спецсимволов и цифр\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Токенизация\n",
    "    words = text.split()\n",
    "    \n",
    "    # Удаление стоп-слов\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Лемматизация\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8555743-ac9d-4f64-ba01-8a9c1bb2d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Загрузка подмножества данных\n",
    "    categories = ['sci.space', 'comp.graphics', 'rec.sport.baseball', 'talk.politics.mideast']\n",
    "    dataset = fetch_20newsgroups(subset='train', categories=categories, \n",
    "                                remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "    texts = dataset.data[:100]  # Используем меньше документов для демонстрации\n",
    "    \n",
    "    # Предварительная обработка текстов\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # Создание матрицы документ-термин\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "    \n",
    "    # Преобразование в список строк для CountVectorizer\n",
    "    text_strings = [\" \".join(tokens) for tokens in processed_texts]\n",
    "    X = vectorizer.fit_transform(text_strings)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return X, feature_names, processed_texts, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0b6bb1-d25c-4211-80dd-190a326a73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_topics(top_words, title=\"Топ-слова тем\"):\n",
    "    n_topics = len(top_words)\n",
    "    n_cols = min(2, n_topics)\n",
    "    n_rows = (n_topics + 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(12, 3 * n_rows))\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    \n",
    "    for topic_idx, words in enumerate(top_words):\n",
    "        plt.subplot(n_rows, n_cols, topic_idx + 1)\n",
    "        # Обратный порядок для отображения самого важного слова сверху\n",
    "        words_sorted = list(reversed(words))\n",
    "        plt.barh(range(len(words_sorted)), [1] * len(words_sorted), align='center')\n",
    "        plt.yticks(range(len(words_sorted)), words_sorted)\n",
    "        plt.title(f\"Тема #{topic_idx + 1}\")\n",
    "        plt.gca().invert_yaxis()  # Инвертируем ось Y для правильного отображения\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(\"lda_topics.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112922ad-5079-46eb-932a-70eddccf6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence(top_words, texts):\n",
    "    # Создание словаря для gensim\n",
    "    from gensim.corpora import Dictionary\n",
    "    gensim_dict = Dictionary(texts)\n",
    "    corpus = [gensim_dict.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # Вычисление когерентности\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=top_words,\n",
    "        texts=texts,\n",
    "        dictionary=gensim_dict,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16cc7ddc-fbce-45ba-9152-d9c504bd0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_from_components(components, feature_names, n_top_words=10):\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        top_words_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words.append([feature_names[i] for i in top_words_indices])\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771e5633-e18d-4a67-94a7-bae428e2209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    # Загрузка и подготовка данных\n",
    "    X, feature_names, processed_texts, _ = load_dataset()\n",
    "    n_topics = 4\n",
    "    \n",
    "    # Результаты\n",
    "    results = {}\n",
    "    \n",
    "    # Обучение и оценка собственной реализации LDA\n",
    "    print(\"\\nОбучение модели: Собственный LDA\")\n",
    "    try:\n",
    "        custom_lda = CustomLDA(n_topics=n_topics, alpha=0.1, beta=0.01, n_iter=100, random_state=42)\n",
    "        start_time = time.time()\n",
    "        custom_lda.fit(X)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Получение топ-слов\n",
    "        custom_top_words = custom_lda.get_top_words(feature_names, n_top_words=10)\n",
    "        \n",
    "        # Оценка когерентности тем\n",
    "        coherence = calculate_coherence(custom_top_words, processed_texts)\n",
    "        \n",
    "        results[\"Собственный LDA\"] = {\n",
    "            'coherence': coherence,\n",
    "            'train_time': train_time,\n",
    "            'model': custom_lda,\n",
    "            'top_words': custom_top_words\n",
    "        }\n",
    "        \n",
    "        print(f\"Собственный LDA завершен. Когерентность: {coherence:.4f}, Время обучения: {train_time:.2f} сек\", custom_top_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обучении Собственный LDA: {str(e)}\")\n",
    "        results[\"Собственный LDA\"] = {\n",
    "            'coherence': np.nan,\n",
    "            'train_time': np.nan,\n",
    "            'model': None,\n",
    "            'top_words': []\n",
    "        }\n",
    "     # Обучение и оценка реализации sklearn LDA\n",
    "    print(\"\\nОбучение модели: Sklearn LDA\")\n",
    "    try:\n",
    "        sklearn_lda = LatentDirichletAllocation(\n",
    "            n_components=n_topics, \n",
    "            doc_topic_prior=0.1, \n",
    "            topic_word_prior=0.01,\n",
    "            learning_method='batch',\n",
    "            max_iter=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        sklearn_lda.fit(X)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Получение топ-слов\n",
    "        sklearn_top_words = get_top_words_from_components(sklearn_lda.components_, feature_names, n_top_words=10)\n",
    "        \n",
    "        # Оценка когерентности тем\n",
    "        coherence = calculate_coherence(sklearn_top_words, processed_texts)\n",
    "        \n",
    "        results[\"Sklearn LDA\"] = {\n",
    "            'coherence': coherence,\n",
    "            'train_time': train_time,\n",
    "            'model': sklearn_lda,\n",
    "            'top_words': sklearn_top_words\n",
    "        }\n",
    "        print(f\"Sklearn LDA завершен. Когерентность: {coherence:.4f}, Время обучения: {train_time:.2f} сек\", sklearn_top_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обучении Sklearn LDA: {str(e)}\")\n",
    "        results[\"Sklearn LDA\"] = {\n",
    "            'coherence': np.nan,\n",
    "            'train_time': np.nan,\n",
    "            'model': None,\n",
    "            'top_words': []\n",
    "        }\n",
    "    \n",
    "    return results, processed_texts, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d084eef6-c945-47b2-9625-58d1fb7122e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обучение модели: Собственный LDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gibbs Sampling: 100%|████████████████████████████████████████████████████████████████| 100/100 [01:08<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственный LDA завершен. Когерентность: 0.5620, Время обучения: 69.15 сек [['space', 'list', 'istanbul', 'post', 'group', 'system', 'email', 'information', 'presentation', 'option'], ['one', 'time', 'last', 'year', 'many', 'three', 'day', 'back', 'mean', 'thing'], ['armenian', 'azerbaijan', 'people', 'said', 'armenia', 'turkey', 'dead', 'turkish', 'village', 'town'], ['would', 'dont', 'like', 'get', 'know', 'could', 'however', 'team', 'see', 'think']]\n",
      "\n",
      "Обучение модели: Sklearn LDA\n",
      "Sklearn LDA завершен. Когерентность: 0.5563, Время обучения: 0.71 сек [['armenian', 'istanbul', 'people', 'turkey', 'dead', 'said', 'turkish', 'like', 'one', 'new'], ['space', 'list', 'post', 'one', 'group', 'would', 'system', 'presentation', 'information', 'email'], ['armenian', 'azerbaijan', 'people', 'azerbaijani', 'attack', 'region', 'april', 'refugee', 'government', 'armenia'], ['option', 'philadelphia', 'would', 'power', 'dont', 'also', 'like', 'module', 'team', 'used']]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results, processed_texts, feature_names = run_experiment()\n",
    "        \n",
    "        # Визуализация тем\n",
    "        for name, res in results.items():\n",
    "            if res.get('top_words'):\n",
    "                visualize_topics(res['top_words'], title=f\"Топ-слова тем ({name})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Критическая ошибка при выполнении эксперимента: {str(e)}\")\n",
    "        results = {}\n",
    "        processed_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea1e7a-c84d-4811-8147-c85551325458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
