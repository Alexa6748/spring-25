# Лабораторная работа №4. Латентное размещение Дирихле

## Задание

1. Выбрать текстовый датасет для анализа, например, на [kaggle](https://www.kaggle.com/datasets).
2. Реализовать алгоритм латентного размещения Дирихле.
3. Обучить модель на выбранном датасете.
4. Оценить качество модели с использованием метрик когерентности тем.
5. Замерить время обучения модели.
6. Сравнить результаты с эталонной реализацией из библиотеки [scikit-learn](https://scikit-learn.org/stable/):
   * когерентность тем;
   * время обучения.
7. Подготовить отчет, включающий:
   * описание алгоритма латентного размещения Дирихле;
   * описание датасета;
   * результаты экспериментов;
   * сравнение с эталонной реализацией;
   * выводы.

## 1. Описание алгоритма LDA

**Latent Dirichlet Allocation (LDA)** — это вероятностная модель, используемая для тематического моделирования коллекций текстовых документов. Модель предполагает, что:

* Каждый документ представляет собой смесь тем.
* Каждая тема — это распределение над словарём слов.
* Генерация слов в документах происходит случайно на основе этих распределений.

**Основные шаги алгоритма:**

* Инициализация случайных тем для каждого слова.
* Повторный проход по словам и перераспределение тем на основе условных вероятностей.
* После нескольких итераций получается устойчивое распределение тем по документам и слов по темам.

Для реализации использовался метод **Gibbs Sampling**.

## 2. Датасет

**Использованный датасет:**

[20 Newsgroups](https://www.kaggle.com/datasets/crawford/20-newsgroups/data)

Выбранное подмножество тем датасета
- компьютерная графика
- электроника
- политика (оружие)
- христианство

**Предобработка:**

Токенизация и удаление стоп-слов с использованием библиотеки nltk

## 3. Реализация LDA

### Инициализация (__init__)

```python
def __init__(self, n_topics, alpha=0.1, beta=0.01, n_iter=20)
```
- n_topics — количество скрытых тем.

- alpha — параметр сглаживания для распределения тем в документе (чем больше, тем более равномерное распределение).

- beta — параметр сглаживания для распределения слов в теме (аналогично, влияет на равномерность).

- n_iter — число итераций Гиббсовского сэмплинга.

Также инициализируются:

- vocab_size — размер словаря.

- doc_topic_counts — матрица "документ × тема" (сколько слов в документе принадлежит каждой теме).

- topic_word_counts — матрица "тема × слово" (сколько раз слово встречается в теме).

- topic_counts — общее количество слов, назначенных в каждую тему.

- assignments — список, хранящий текущие назначения тем для каждого слова.



### Как работает Gibbs Sampling в LDA

**Gibbs Sampling** — это алгоритм из семейства методов Монте-Карло по схеме цепей Маркова, используемый для генерации выборки из многомерного распределения вероятностей, когда прямая выборка затруднена.

В контексте LDA Gibbs Sampling используется для оценки скрытых тем в документах и распределений слов по темам.

1. Каждому слову в каждом документе случайно назначается тема.
2. Для каждого слова:
   * Удаляется текущая тема из счётчиков.
   * Вычисляется вероятность принадлежности слова к каждой теме на основе:
     * частоты этой темы в документе
     * частоты слова в этой теме 
   * Назначается новая тема согласно вычисленной вероятности.
3. Процесс повторяется много раз (итераций), пока модель не "сойдётся".

## 4. Эталонная реализация (sklearn)

**Модель:**

```python
from sklearn.decomposition import LatentDirichletAllocation
```

**Подготовка данных:**

* Векторизация с помощью `CountVectorizer`
* Обучение LDA на матрице TF (term frequency)

## 5. Сравнение и оценка качества

| Реализация | Время обучения | Когерентность |
|------------|----------------|---------------|
| Custom     | 48.34          | 0.574         |
| Sklearn    | 7.06           | 0.543         |

