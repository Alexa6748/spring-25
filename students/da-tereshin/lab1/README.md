# Лабораторная работа №1. Ансамбли моделей

## Датасет

### National Health and Nutrition Health Survey 2013-2014 (NHANES) Age Prediction Subset

URL : https://archive.ics.uci.edu/dataset/887/national+health+and+nutrition+health+survey+2013-2014+(nhanes)+age+prediction+subset

Датасет был собран в рамках Национального обследования здоровья и питания (NHANES), проводимого Центрами по контролю и профилактике заболеваний (CDC). В ходе исследования была собрана обширная информация о состоянии здоровья и питании различных групп населения США.

Сбор данных осуществлялся путем опроса, обследования и лабораторных исследований.

Данных датасет предоставляется для решения задачи классификации респондентов с целью определения их возрастной группы: "пожилые" (от 65 лет) и "не пожилые" (до 65 лет)

### Признаки

1. SEQN - ID респондента (числовое)
2. RIDAGEYR - реальный возраст респондента (числовое)
3. RIAGENDR - пол (1-Male, 2-Female)
4. PAQ605 -  показатель, показывающий если респондент занимается спортом, фитнесом или рекреационной деятельностью умеренной или энергичной интенсивности в течение обычной недели (1 - еженедельные физические нагрузки высокой интенсивности, 2 - их отсутствие)
5. BMXBMI - индекс массы тела респондента (числовое)
6. LBXGLU - уровень глюкозы в крови после голодания (числовое)
7. DIQ010 - страдает ли респондент диабетом
8. LBXGLT - Respondent's Oral (числовое)
9. LBXIN - уровень инсулина в крови (числовое)
10. **age_group** - таргет ('Adult' или 'Senior')

## Выбранный ансамблевый метод

Bagging (англ. bootstrap aggregating) – метод ансамблированя, при котором происходит обучение базовых алгоритмов 
на бустрэп-выборках.

Бутстрэп-выборка – выборка, получаемая из исходного набора данных путем случайного выбора элементов с возвращением. 
Обычно каждая бутстрэп-выборка имеет тот же размер, что и исходный набор данных, 
и может содержать повторяющиеся элементы.
Если мы рассматриваем бесконечно большое количество бутстрэп-выборок, то в среднем около 63.2%  элементов исходного 
набора данных будут включены в каждую бутстрэп-выборку.

Для классификации элемента выбирается мажоритарный класс по всем предсказаниям алгоритмов в ансамбле.



## Результаты обучения

### Время обучения

| Количество алгоритмов (n_estimators) | Своя реализация, мс | Библиотечная реализация, мс |
|--------------------------------------|---------------------|-----------------------------|
| 10                                   | 90.9 ± 4.05         | 96.4 ± 4.14                 |
| 50                                   | 566 ± 45            | 503 ± 66.7                  |
| 100                                  | 924 ± 34.7          | 914 ± 66.2                  |

### Точность моделей



<table>
    <thead>
        <tr>
            <th rowspan="2">Количество алгоритмов (n_estimators)</th>
            <th colspan="3">Своя реализация</th>
            <th colspan="3">Библиотечная реализация</th>
        </tr>
        <tr>
            <th>Accuracy</th>
            <th>f1-score класса 0</th>
            <th>f1-score класса 1</th>
            <th>Accuracy</th>
            <th>f1-score класса 0</th>
            <th>f1-score класса 1</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>10</td>
            <td>0.79</td>
            <td>0.37</td>
            <td>0.88</td>
            <td>0.78</td>
            <td>0.30</td>
            <td>0.87</td>
        </tr>
        <tr>
            <td>50</td>
            <td>0.81</td>
            <td>0.34</td>
            <td>0.89</td>
            <td>0.82</td>
            <td>0.31</td>
            <td>0.89</td>
        </tr>
        <tr>
            <td>100</td>
            <td>0.83</td>
            <td>0.35</td>
            <td>0.90</td>
            <td>0.83</td>
            <td>0.32</td>
            <td>0.90</td>
        </tr>
    </tbody>
</table>


## Выводы

Поскольку своя реализация в качестве базового алгоритма использует готовый библиотечный класс классификатора, то время обучения 
ансамблей почти не отличается при малом n_estimators в рамках погрешности (время для готового ансамбля даже слегка больше). При 
увеличении числа базовых алгоритмов библиотечная реализация бэггинга без параллелизации (параметр n_jobs=None) начинает
обучаться быстрее, что можно объяснить внутренними оптимизациями и эффективным использованием памяти.

При использовании нескольких десятков базовых алгоритмов в ансамблях точность обеих реализаций совпадает. При меньшем 
количестве алгоритмов разницу вносит стохастика при составлении бутстрэп-выборок. 