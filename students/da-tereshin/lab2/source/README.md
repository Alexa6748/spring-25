# Лабораторная работа №2. Градиентный бустинг

## Датасет

### Abalone

URL: https://archive.ics.uci.edu/dataset/1/abalone

Датасет предназначен для прогнозирования возраста абалонов по физическим измерениям.  Возраст абалона определяют, разрезая раковину по конусу, окрашивая ее и подсчитывая количество колец через микроскоп - скучная и трудоемкая задача. Для прогнозирования возраста используются другие измерения, которые легче получить.  Для решения задачи может потребоваться дополнительная информация, например, погодные условия и местоположение (следовательно, наличие пищи).

**Признаки**

1. Sex – пол (категориальный), M/F/I (младенец)
2. Length – максимальная длина раковины (мм)
3. Diameter	– перпендикуляр к длине (мм)
4. Height – ширина с мясом в оболочке (мм)
5. Whole weight – вес всей ракушки (грамм)
6. Shucked weight – вес внутренностей (грамм)
7. Viscera weight –  вес кишечника (грамм)
8. Shell weight – вес сухой ракушки (грамм)
9. **Rings** – таргет, количество колец (+1,5 дает возраст в годах)

## Описание алгоритма

Градиентный бустинг – обобщение алгоритма бустинга для произвольной дифференцируемой функции потерь.
Представляет собой метод ансамблевого обучения, который строит модель, добавляя новые базовые алгоритмы
для минимизации ошибки предыдущих алгоритмов. Основная идея заключается в том, чтобы последовательно обучать слабые модели
ошибках предыдущих моделей, чтобы в конечном итоге получить сильную модель с высокой точностью предсказаний.

В ходе последовательного обучения на каждом шаге:
1. Вычисляется градиент функции потерь (ошибка предыдущей модели).
2. Новый базовый алгоритм обучается на антиградиентах функции потерь.
3. Предсказания новой модели умножаются на скорость обучения и добавляются к предсказаниям предыдущей модели.

Финальная модель представляет собой взвешенную сумму всех базовых моделей, где веса определяются скоростью обучения.

Для решения задачи многоклассовой классификации в моем решении используется подход One-vs-all (или one-vs-rest), то есть на
каждой итерации обучается n базовых классификаторов, где каждый учится только на своем классе и решает задачу бинарной
классификации.

## Результаты обучения

### Время обучения

| Количество алгоритмов (n_estimators) | Своя реализация, мс | Библиотечная реализация, мс |
|--------------------------------------|---------------------|-----------------------------|
| 10                                   | 53.9 ± 4.29         | 53 ± 2.54                   |
| 50                                   | 304 ± 41.3          | 274 ± 34.8                  |
| 100                                  | 601 ± 41.7          | 526 ± 14                    |

### Точность моделей

<table>
    <thead>
        <tr>
            <th rowspan="2">Количество алгоритмов (n_estimators)</th>
            <th colspan="4">Своя реализация</th>
            <th colspan="4">Библиотечная реализация</th>
        </tr>
        <tr>
            <th>MSE train</th>
            <th>MSE test</th>
            <th>R2 train</th>
            <th>R2 test</th>
            <th>MSE train</th>
            <th>MSE test</th>
            <th>R2 train</th>
            <th>R2 test</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>10</td>
            <td>5.8456</td>
            <td>6.6995</td>
            <td>0.4291</td>
            <td>0.3902</td>
            <td>5.8456</td>
            <td>6.6995</td>
            <td>0.4291</td>
            <td>0.3902</td>
        </tr>
        <tr>
            <td>50</td>
            <td>3.9775</td>
            <td>5.119</td>
            <td>0.6115</td>
            <td>0.5341</td>
            <td>3.9775</td>
            <td>5.1277</td>
            <td>0.6115</td>
            <td>0.5333</td>
        </tr>
        <tr>
            <td>100</td>
            <td>3.4898</td>
            <td>4.9561</td>
            <td>0.6592</td>
            <td>0.5489</td>
            <td>3.4898</td>
            <td>4.9261</td>
            <td>0.6592</td>
            <td>0.5516</td>
        </tr>
    </tbody>
</table>


## Выводы

Вне зависимости от количества базовых алгоритмов точность реализаций совпадает на обучающей выборке и почти совпадает на
тестовой (имеется погрешность при большом n_estimators в районе сотых).
При увеличении количества базовых алгоритмов увеличивается разность в скорости обучения – библиотечная реализация работает
чуть быстрее.