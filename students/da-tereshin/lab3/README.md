# Лабораторная работа №3. Наивный байесовский классификатор

## Датасет

URL: https://archive.ics.uci.edu/dataset/109/wine

Эти данные являются результатом химического анализа вин, выращенных в одном и том же регионе Италии, но полученных из
трех разных сортов. В ходе анализа было определено количество 13 компонентов, содержащихся в каждом из трех типов вин.

**Признаки**

1) Alcohol – содержание алкоголя в вине
2) Malic acid – уровень яблочной кислоты
3) Ash – содержание минеральных веществ в вине, оставшихся после сжигания
4) Alcalinity of ash – уровень щелочности минеральных веществ в золе
5) Magnesium – содержание магния в вине
6) Total phenols – общее содержание фенольных соединений
7) Flavanoids – содержание флаваноидов, которые являются подгруппой фенольных соединений
8) Nonflavanoid phenols – нефлаваноидные фенолы
9) Proanthocyanins – проантоцианидины
10) Color intensity – интенсивность цвета
11) Hue – оттенок цвета
12) OD280/OD315 of diluted wines – отношение оптической плотности при длинах волн 280 нм и 315 нм
13) Proline – cодержание аминокислоты пролина
14) **Class** – целевая переменная, один из трех сортов вина (0,1,2)

## Описание алгоритма
Наивный байесовский классификатор основан на теореме Байеса и предположении о независимости признаков.
Это означает, что каждый признак вносит вклад в классификацию независимо от других признаков.

Согласно этой теореме апостериорную вероятность класса C при наличии признаков X можно вычислить через
произведение априорной вероятности класса C на вероятность наблюдения признаков X при условии, что класс C истинный.

Предположение о независимости признаков позволяет упростить вычисления и представить P(X|C) как произведение вероятностей 
наблюдения каждого из признаков при условии класса C.

Обучение модели происходит в несколько шагов:
1. Вычисление априорных вероятностей классов
2. Вычисление вероятностей признаков при условии класса
3. Классификация нового примера (пример принадлежит к классу C, для которого апостериорная вероятность максимальна)

## Результаты обучения

### Время обучения

| Своя реализация, мс | Библиотечная реализация, мс |
|---------------------|-----------------------------|
| 0.388 ± 0.0274      | 3.3 ± 0.46                  |

### Точность моделей

<table>
    <thead>
        <tr>
            <th colspan="4">Своя реализация</th>
            <th colspan="4">Библиотечная реализация</th>
        </tr>
        <tr>
            <th>Accuracy train</th>
            <th>F1 avg train</th>
            <th>Accuracy test</th>
            <th>F1 avg test</th>
            <th>Accuracy train</th>
            <th>F1 avg train</th>
            <th>Accuracy test</th>
            <th>F1 avg test</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>0.99</td>
            <td>0.99</td>
            <td>0.97</td>
            <td>0.98</td>
            <td>0.99</td>
            <td>0.99</td>
            <td>0.97</td>
            <td>0.98</td>
        </tr>
    </tbody>
</table>

## Выводы

На небольшом датасете обе реализации показывают одинаково высокие результаты многоклассовой классификации. Своя реализация
работает быстрее библиотечной в 8.5 раз, поскольку использует упрощенную реализацию и не осуществляет дополнительные 
проверки и оптимизации. На больших датасетах эта разница способна значительно сократиться.