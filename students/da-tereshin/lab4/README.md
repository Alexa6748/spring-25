# Лабораторная работа №4. Латентное размещение Дирихле

## Датасет 

URL: https://www.kaggle.com/datasets/jacopoferretti/bbc-articles-dataset

Датасет содержит 2225 статей с новостного сайта BBC за 2004-2005г. , разделенных на 5 тематических областей:
1. sport
2. business
3. politics
4. tech
5. entertainment

## Описание алгоритма
Алгоритм Латентного Размещения Дирихле (LDA) — это вероятностная модель, используемая для выявления скрытых
тем в большом наборе текстовых документов. В отличие от алгоритма pLSA LDA дополнительно предполагает, что векторы тематик
и документов порождаются распределением Дирихле.

Для идентификации параметров тематической модели по коллекции документов применяется принцип максимизации правдоподобия.
Для решения оптимизационной задачи используется EM-алгоритм.

Алгоритм получает на вход коллекцию текстовых документов. На выходе для каждого документа выдается числовой вектор,
составленный из оценок степени принадлежности данного документа каждой из тем.

Предположения для LDA:
* Документы с похожими темами используют похожие группы слов.
* Скрытые темы можно найт путем поиска групп слов, которые часто встречаются вместе в документах по всему корпусу.
* Документы – это распределения вероятностей по скрытым темам, что означает, что определенный документ будет содержать 
больше слов определенной темы.
* Сами темы – это распределения вероятностей по словам.

Для идентификации параметров тематической модели по коллекции документов применяется сэмплирование Гиббса или 
вариационный байесовский вывод. Поскольку в библиотеке sklearn используется последний вариант, своя реализация также 
использует variational Bayes (VB).

## Результаты обучения

### Время обучения

| Своя реализация, c | Библиотечная реализация, с |
|-------------------|----------------------------|
| 13.3  ± 0.169     | 3.4  ± 0.039               |

### Когерентность тем
Когерентность оценивает, насколько слова внутри одной тематики семантически связаны между собой.
Именно она чаще всего коррелирует с экспертной оценкой.

Для вычисления когерентности использовалась библиотека gensim.

| Своя реализация  | Библиотечная реализация  |
|------------------|--------------------------|
| 0.7467           | 0.7430                   |


## Выводы

На выбранном датасете обе реализации показывают очень схожие значения когерентности.
При этом библиотечная реализация работает в разы быстрее, так как эффективно обрабатывает разреженные матрицы. В своей 
реализации X преобразовывается в плотную матрицу, что значительно увеличивает время обучения.