# Отчет по лабораторной работе

## Описание модели латентных факторов

**Модель латентных факторов** - это метод коллаборативной фильтрации для рекомендательных систем, основанный на матричной факторизации.

### Теоретические основы:

**Принцип работы:**
- Представляет пользователей и элементы (фильмы) в пространстве скрытых факторов низкой размерности
- Каждый пользователь и фильм описывается вектором латентных факторов
- Рейтинг предсказывается как скалярное произведение векторов пользователя и фильма плюс смещения

### Формула
rating = global_bias + user_bias + movie_bias + user_factors * movie_factors

где:
- global_bias — общее смещение (средний рейтинг по всему датасету)
- user_bias — индивидуальное смещение пользователя
- movie_bias — смещение конкретного фильма
- user_factors, movie_factors — векторы латентных факторов

### Обучение:
Модель обучается методом стохастического градиентного спуска с регуляризацией для предотвращения переобучения.

## Описание датасета

**Источник:** https://www.kaggle.com/datasets/aigamer/movie-lens-dataset

**Характеристики:** 
- Пользователи: 610 уникальных
- Фильмы: 9,724 уникальных
- Рейтинги: 100,836 оценок
- Шкала рейтингов: 0.5 - 5.0
- Разреженность матрицы: 98.30%

### Статистика рейтингов:
- Средний рейтинг: 3.50
- Стандартное отклонение: 1.04
- Самый популярный рейтинг: 4.0 (26.6% всех оценок)

## Результаты экспериментов и выводы
### Модель 1
- n_factors=50,
- learning_rate=0.01,
- regularization=0.1,
- n_epochs=100,
- random_state=42
- Время обучения: 77.96 секунд
- Train RMSE: 0.5177
- Test RMSE: 0.8584

### Модель 2 (регуляризация в 10 раз меньше)
- n_factors=50,
- learning_rate=0.01,
- regularization=0.01,
- n_epochs=100,
- random_state=42
- Время обучения: 79.20 секунд
- Train RMSE: 0.1476
- Test RMSE: 0.9604

### Модель 3 (регуляризация в 2 раза меньше, чем в Модели 1)
- n_factors=50,
- learning_rate=0.01,
- regularization=0.05,
- n_epochs=100,
- random_state=42
- Время обучения: 79.20 секунд
- Train RMSE: 0.3460
- Test RMSE: 0.8844

### Эксперимент с количеством факторов
Диапазон: 20 - 80

#### Факторов: 20
- Train RMSE: 0.5573 | Test RMSE: 0.8674 | - Gap: 0.3101 | Время: 74.5с

#### Факторов: 30
- Train RMSE: 0.5306 | Test RMSE: 0.8609 | - Gap: 0.3303 | Время: 64.7с

#### Факторов: 40
- Train RMSE: 0.5156 | Test RMSE: 0.8594 | - Gap: 0.3437 | Время: 65.3с

#### Факторов: 50
- Train RMSE: 0.5080 | Test RMSE: 0.8584 | - Gap: 0.3504 | Время: 68.4с

#### Факторов: 60
- Train RMSE: 0.4999 | Test RMSE: 0.8574 | - Gap: 0.3575 | Время: 66.1с

#### Факторов: 70
- Train RMSE: 0.4958 | Test RMSE: 0.8578 | - Gap: 0.3620 | Время: 74.7с

#### Факторов: 80
- Train RMSE: 0.4928 | Test RMSE: 0.8586 | - Gap: 0.3658 | Время: 74.8с

### Лучшая модель
- Факторов: 60
- Test RMSE: 0.8574

### Сравнение с suprise SVD
- Использовались параметры лучшей самописной модели
- Train RMSE: 0.5016
- Test RMSE: 0.8586
- Время обучения: 3.60с

### Ключевые результаты:

1. **Качество рекоммендаций:**
   - Собственная реализация оказалась точнее

2. **Производительность:**
   - Собственная реализация ожидаемо работает медленнее surpise

### Практические выводы:

1. **Применимость:**
- Алгоритм одходит для персонализированных рекоммендаций
- Можно протестировать на большем количестве данных и других датасетах

2. **Ограничения:**
- Скорость работы, для крупных коммерческих систем понадобится оптимизация
- Присутствует переобучение, требуется настройка реугляризации и количества факторов