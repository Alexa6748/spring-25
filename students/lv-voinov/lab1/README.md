# Лабораторная работа 1

## Датасет

Breast Cancer

<https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset>

Датасет для бинарной классификации. 30 числовых признаков, доля класса 1 63%. Качественный датасет, содержащий малое количество шума.

## Алгоритм бустинга AdaBoost

Алгоритм AdaBoost (Adaptive Boosting) — это ансамблевый алгоритм машинного обучения, который строит сильный классификатор путём комбинирования последовательно обученных слабых классификаторов (в данном случае используются решающие пни - решающие деревья глубиной 1).

Начальным этапом алгоритма является инициализация весов для каждого обучающего примера: все веса устанавливаются равными, что означает, что изначально каждый объект имеет одинаковую важность. Затем алгоритм вступает в итеративный процесс. На каждой итерации обучается новый слабый классификатор, который фокусируется на примерах, неправильно классифицированных предыдущими моделями. Для этого используется текущее распределение весов объектов.

После обучения вычисляется ошибка классификатора, которая учитывает веса ошибочно предсказанных примеров. Если ошибка превышает порог случайного угадывания, процесс может прерываться. На основе ошибки определяется вес (значимость) текущего слабого классификатора в итоговом ансамбле: чем меньше ошибка, тем больший вес присваивается классификатору. Формула для вычисления этого веса обычно включает логарифмическое отношение точности классификатора.

Далее происходит обновление весов обучающих примеров: веса неправильно классифицированных объектов увеличиваются, а корректно распознанных — уменьшаются, что заставляет следующий слабый классификатор больше концентрироваться на "трудных" примерах. После обновления веса нормализуются, чтобы их сумма оставалась равной единице. Этот цикл повторяется заданное количество раз или до достижения достаточной точности.

В итоге сильный классификатор формируется как взвешенная сумма решений всех слабых классификаторов, где вес каждого отражает его эффективность. Классификация нового объекта выполняется путём вычисления знака этой суммы. AdaBoost адаптивно улучшает ансамбль, минимизируя экспоненциальную функцию потерь, что делает его эффективным для задач бинарной классификации, но чувствительным к шуму в данных.

### Код алгоритма

```python
class AdaBoost:
    def __init__(self, n_estimators):
        self.n_estimators = n_estimators
        self.models = []
        self.alphas = []

    def fit(self, X, y):
        n_samples = X.shape[0]
        weights = np.ones(n_samples) / n_samples
        
        for _ in range(self.n_estimators):
            model = DecisionTreeClassifier(max_depth=1)
            model.fit(X, y, sample_weight=weights)
            pred = model.predict(X)
            
            incorrect = (pred != y)
            error = np.dot(weights, incorrect)
            
            if error >= 0.5 - 1e-10:
                break
            if error < 1e-10:
                alpha = 1e5
            else:
                alpha = 0.5 * np.log((1 - error) / error)
            
            weights *= np.exp(alpha * incorrect)
            weights /= np.sum(weights)
            
            self.models.append(model)
            self.alphas.append(alpha)

    def predict(self, X):
        preds = np.array([model.predict(X) for model in self.models])
        weighted_sum = np.dot(self.alphas, preds)
        return np.sign(weighted_sum)
```

### Результаты работы ручного алгоритма

Результаты и время работы, посчитанные с помощью кросс-валидации для различных n_estimators:

```
Mean accuracy for n_estimators=25: 0.9788 (+-0.0120), time: 0:00:00.354070
Mean accuracy for n_estimators=50: 0.9841 (+-0.0117), time: 0:00:00.750160
Mean accuracy for n_estimators=75: 0.9858 (+-0.0132), time: 0:00:01.122841
Mean accuracy for n_estimators=100: 0.9894 (+-0.0142), time: 0:00:01.438270
Mean accuracy for n_estimators=150: 0.9894 (+-0.0142), time: 0:00:02.075900
Mean accuracy for n_estimators=200: 0.9912 (+-0.0112), time: 0:00:02.856496
Mean accuracy for n_estimators=250: 0.9912 (+-0.0112), time: 0:00:03.476977
```

### Результаты работы библиотечного алгоритма

Результаты и время работы, посчитанные с помощью кросс-валидации для различных n_estimators:

```
Mean accuracy for n_estimators=25: 0.9596 (+-0.0142), time: 0:00:00.458288
Mean accuracy for n_estimators=50: 0.9684 (+-0.0131), time: 0:00:00.931951
Mean accuracy for n_estimators=75: 0.9719 (+-0.0102), time: 0:00:01.371355
Mean accuracy for n_estimators=100: 0.9772 (+-0.0105), time: 0:00:01.755123
Mean accuracy for n_estimators=150: 0.9754 (+-0.0102), time: 0:00:02.557938
Mean accuracy for n_estimators=200: 0.9772 (+-0.0089), time: 0:00:03.620275
Mean accuracy for n_estimators=250: 0.9772 (+-0.0105), time: 0:00:04.592839
```

### Выводы

В результате экспериментов было выяснено, что оптимальное количество классификаторов - 200. Ручной алгоритм достигает точности 0.9912, что на 0.014 выше, чем у библиотечного. Время работы ручного алгоритма примерно в 1.3 раза меньше, чем у библиотечного.

Таким образом, реализованный алгоритм эффективнее для классификации на выбранном датасете, чем алгоритм из библиотеки.
