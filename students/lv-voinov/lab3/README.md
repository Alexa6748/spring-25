# Лабораторная работа 3

## Датасет

Breast Cancer

<https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset>

Датасет для бинарной классификации. 30 числовых признаков, доля класса 1 63%. Качественный датасет, содержащий малое количество шума.

## Наивный байесовский классификатор

Наивный байесовский классификатор — это алгоритм машинного обучения, основанный на теореме Байеса, а также предположении об условной независимости признаков (наивное предположение). Он широко используется для задач классификации благодаря простоте и эффективности, особенно в обработке текстов, спам-фильтрации и рекомендательных системах.

Для обучения модели вычисляются априорные вероятности классов P(C) на основе частоты классов в обучающих данных, затем для каждого признака Xi и класса C определяются условные вероятности P(Xi∣C).

Чтобы классифицировать новый объект, для объекта с признаками X=(x1,...,xn) рассчитывается апостериорная вероятность для каждого класса с помощью априорной вероятности и вычисления правдоподобия. После этого выбирается класс с наибольшей вероятностью.

Существует мультиномиальный наивный Байес, который используется для категориальных данных, в нем используется сглаживание Лапласа. Также существует гауссовский наивный Байес, который используется для непрерывных данных и предполагает, что признаки распределены нормально. Этот алгоритм реализован в данной работе.

### Код алгоритма

```python
class GaussianNaiveBayes:
    def __init__(self):
        self.classes = None
        self.priors = {}
        self.mean = {}
        self.var = {}

    def fit(self, X, y):
        self.classes = np.unique(y)

        for cls in self.classes:
            X_cls = X[y == cls]
            self.priors[cls] = X_cls.shape[0] / X.shape[0]
            self.mean[cls] = X_cls.mean(axis=0)
            self.var[cls] = X_cls.var(axis=0)

    def predict(self, X):
        predictions = []
        for x in X:
            posteriors = []
            for cls in self.classes:
                prior = np.log(self.priors[cls])
                likelihood = np.sum(np.log(norm.pdf(x, loc=self.mean[cls], scale=np.sqrt(self.var[cls])) + 1e-9))
                posterior = prior + likelihood
                posteriors.append(posterior)
            best_class = self.classes[np.argmax(posteriors)]
            predictions.append(best_class)
        return predictions
```

### Результаты работы

Результаты работы ручного алгоритма (кросс-валидация, k=5):

```
Mean accuracy: 0.9363 (+-0.0172), time: 0:00:00.107992
```

Результаты работы библиотечного алгоритма (кросс-валидация, k=5):

```
Mean accuracy: 0.9279 (+-0.0204), time: 0:00:00.017666
```

### Выводы

Ручной байесовский классификатор показывает более хорошие результаты, чем библиотечный (на 0.01 лучше), но работает в 6 раз медленнее.
