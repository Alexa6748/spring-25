{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024648a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as SklearnLDA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAGibbs:\n",
    "    def __init__(self, n_topics=10, alpha=0.1, beta=0.1, max_iter=100):\n",
    "        self.n_topics = n_topics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.max_iter = max_iter\n",
    "        self.vocab = None\n",
    "        self.phi = None\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, X, vocab):\n",
    "        n_docs = X.shape[0]\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        self.n_dk = np.zeros((n_docs, self.n_topics)) + self.alpha\n",
    "        self.n_kw = np.zeros((self.n_topics, self.vocab_size)) + self.beta\n",
    "        self.n_k = np.zeros(self.n_topics) + self.vocab_size * self.beta\n",
    "        \n",
    "        self.z = []\n",
    "        for d in range(n_docs):\n",
    "            doc = X[d].tocoo()\n",
    "            cols = doc.col\n",
    "            data = doc.data\n",
    "            word_indices = np.repeat(cols, data)\n",
    "            n_words_in_doc = len(word_indices)\n",
    "            \n",
    "            topics_in_doc = np.random.choice(self.n_topics, size=n_words_in_doc)\n",
    "            self.z.append(topics_in_doc)\n",
    "            \n",
    "            for w, topic in zip(word_indices, topics_in_doc):\n",
    "                self.n_dk[d, topic] += 1\n",
    "                self.n_kw[topic, w] += 1\n",
    "                self.n_k[topic] += 1\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            for d in range(n_docs):\n",
    "                doc = X[d].tocoo()\n",
    "                cols = doc.col\n",
    "                data = doc.data\n",
    "                word_indices = np.repeat(cols, data)\n",
    "                topics_in_doc = self.z[d]\n",
    "                \n",
    "                for i in range(len(word_indices)):\n",
    "                    w = word_indices[i]\n",
    "                    old_topic = topics_in_doc[i]\n",
    "                    \n",
    "                    self.n_dk[d, old_topic] -= 1\n",
    "                    self.n_kw[old_topic, w] -= 1\n",
    "                    self.n_k[old_topic] -= 1\n",
    "                    \n",
    "                    p_topics = (self.n_dk[d] / (self.n_dk[d].sum() + self.n_topics * self.alpha)) * \\\n",
    "                               (self.n_kw[:, w] / (self.n_k + 1e-12))\n",
    "                    p_topics = p_topics / p_topics.sum()\n",
    "                    \n",
    "                    new_topic = np.random.choice(self.n_topics, p=p_topics)\n",
    "                    topics_in_doc[i] = new_topic\n",
    "                    \n",
    "                    self.n_dk[d, new_topic] += 1\n",
    "                    self.n_kw[new_topic, w] += 1\n",
    "                    self.n_k[new_topic] += 1\n",
    "        \n",
    "        self.phi = self.n_kw / self.n_k[:, np.newaxis]\n",
    "        self.theta = (self.n_dk) / (self.n_dk.sum(axis=1)[:, np.newaxis] + 1e-12)\n",
    "        return self\n",
    "    \n",
    "    def get_topics(self, n_words=10):\n",
    "        topic_words = []\n",
    "        for k in range(self.n_topics):\n",
    "            top_indices = self.phi[k].argsort()[-n_words:][::-1]\n",
    "            topic_words.append([self.vocab[i] for i in top_indices])\n",
    "        return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence(topics, X, vocab):\n",
    "    word2id = {word: idx for idx, word in enumerate(vocab)}\n",
    "    doc_count = np.zeros(len(vocab))\n",
    "    co_doc_count = defaultdict(int)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        doc = X[i].tocoo()\n",
    "        words = set(doc.col)\n",
    "        for w in words:\n",
    "            doc_count[w] += 1\n",
    "        word_list = list(words)\n",
    "        for i1 in range(len(word_list)):\n",
    "            for i2 in range(i1+1, len(word_list)):\n",
    "                w1, w2 = word_list[i1], word_list[i2]\n",
    "                if w1 > w2:\n",
    "                    w1, w2 = w2, w1\n",
    "                co_doc_count[(w1, w2)] += 1\n",
    "    \n",
    "    coherence = 0\n",
    "    topic_count = 0\n",
    "    for topic in topics:\n",
    "        topic_words = [word for word in topic if word in word2id]\n",
    "        if len(topic_words) < 2:\n",
    "            continue\n",
    "            \n",
    "        topic_coherence = 0\n",
    "        pair_count = 0\n",
    "        word_ids = [word2id[w] for w in topic_words]\n",
    "        \n",
    "        for i in range(1, len(word_ids)):\n",
    "            for j in range(i):\n",
    "                w1, w2 = word_ids[j], word_ids[i]\n",
    "                if w1 > w2:\n",
    "                    w1, w2 = w2, w1\n",
    "                count = co_doc_count.get((w1, w2), 1e-5)\n",
    "                topic_coherence += np.log((count + 1e-5) / (doc_count[w2] + 1e-5))\n",
    "                pair_count += 1\n",
    "        \n",
    "        if pair_count > 0:\n",
    "            coherence += topic_coherence / pair_count\n",
    "            topic_count += 1\n",
    "    \n",
    "    return coherence / topic_count if topic_count > 0 else 0\n",
    "\n",
    "def preprocess_data():\n",
    "    categories = [\n",
    "        'comp.windows.x',\n",
    "        'misc.forsale',\n",
    "        'rec.autos',\n",
    "        'rec.sport.hockey',\n",
    "        'sci.crypt',\n",
    "        'sci.electronics',\n",
    "        'sci.med',\n",
    "        'sci.space',\n",
    "        'soc.religion.christian',\n",
    "        'talk.politics.guns']\n",
    "    newsgroups = fetch_20newsgroups(\n",
    "        subset='all', categories=categories,\n",
    "        remove=('headers', 'footers', 'quotes')\n",
    "    )\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    X = vectorizer.fit_transform(newsgroups.data)\n",
    "    return X, vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb70572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X, vocab, n_topics=10, max_iter=20):\n",
    "    _, X_sample, _, _ = train_test_split(X, np.zeros(X.shape[0]), test_size=0.1, random_state=42)\n",
    "    \n",
    "    start = time.time()\n",
    "    our_lda = LDAGibbs(n_topics=n_topics, max_iter=max_iter).fit(X_sample, vocab)\n",
    "    our_time = time.time() - start\n",
    "    our_topics = our_lda.get_topics(n_words=10)\n",
    "    our_coherence = calculate_coherence(our_topics, X_sample, vocab)\n",
    "    \n",
    "    start = time.time()\n",
    "    sklearn_lda = SklearnLDA(\n",
    "        n_components=n_topics,\n",
    "        learning_method='batch',\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "    sklearn_lda.fit(X_sample)\n",
    "    sklearn_time = time.time() - start\n",
    "    \n",
    "    sklearn_topics = []\n",
    "    for topic_weights in sklearn_lda.components_:\n",
    "        top_indices = topic_weights.argsort()[-10:][::-1]\n",
    "        sklearn_topics.append([vocab[i] for i in top_indices])\n",
    "    \n",
    "    sklearn_coherence = calculate_coherence(sklearn_topics, X_sample, vocab)\n",
    "    \n",
    "    return {\n",
    "        'our_time': our_time,\n",
    "        'our_coherence': our_coherence,\n",
    "        'sklearn_time': sklearn_time,\n",
    "        'sklearn_coherence': sklearn_coherence,\n",
    "        'our_topics': our_topics,\n",
    "        'sklearn_topics': sklearn_topics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ce34e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, vocab = preprocess_data()\n",
    "\n",
    "results = compare_models(X, vocab, n_topics=10, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bff90ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ручной LDA:\n",
      "  Время обучения: 51.58 сек\n",
      "  Когерентность тем: -2.1486\n",
      "Тема 1: entry, output, use, program, file\n",
      "Тема 2: god, people, don, say, think\n",
      "Тема 3: right, know, like, power, think\n",
      "Тема 4: server, work, using, time, use\n",
      "Тема 5: file, number, program, information, oname\n",
      "Тема 6: cancer, people, group, book, just\n",
      "Тема 7: car, think, game, vitamin, good\n",
      "Тема 8: edu, keyboard, pc, available, xfree86\n",
      "Тема 9: 00, 10, government, new, 20\n",
      "Тема 10: like, want, know, just, way\n",
      "\n",
      "Sklearn LDA:\n",
      "  Время обучения: 5.61 сек\n",
      "  Когерентность тем: -2.4519\n",
      "Тема 1: entry, use, like, xfree86, file\n",
      "Тема 2: output, file, government, people, like\n",
      "Тема 3: cancer, clutch, people, hiv, information\n",
      "Тема 4: 00, 10, 50, 1st, 15\n",
      "Тема 5: like, know, people, does, just\n",
      "Тема 6: god, know, want, don, think\n",
      "Тема 7: keyboard, like, pc, new, price\n",
      "Тема 8: gm, game, 03, team, 02\n",
      "Тема 9: 10, 00, people, right, don\n",
      "Тема 10: vitamin, retinol, use, liver, time\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ручной LDA:\")\n",
    "print(f\"  Время обучения: {results['our_time']:.2f} сек\")\n",
    "print(f\"  Когерентность тем: {results['our_coherence']:.4f}\")\n",
    "for i, topic in enumerate(results['our_topics']):\n",
    "    print(f\"Тема {i+1}: {', '.join(topic[:5])}\")\n",
    "\n",
    "print(\"\\nSklearn LDA:\")\n",
    "print(f\"  Время обучения: {results['sklearn_time']:.2f} сек\")\n",
    "print(f\"  Когерентность тем: {results['sklearn_coherence']:.4f}\")\n",
    "for i, topic in enumerate(results['sklearn_topics']):\n",
    "    print(f\"Тема {i+1}: {', '.join(topic[:5])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
