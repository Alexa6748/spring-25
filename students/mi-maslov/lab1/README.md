# Лабораторная работа №1. Ансамбли моделей

## Описание работы

В рамках данной лабораторной работы были реализованы и исследованы три ансамблевых метода машинного обучения:
- **Бустинг (AdaBoost)**
- **Бэггинг (Bagging)**
- **Метод случайных подпространств (Random Subspace Method, RSM)**

Все методы были реализованы с нуля и сравнены с эталонными реализациями из библиотеки scikit-learn.

## Описание датасета

Для экспериментов использовался классический датасет **Iris** из библиотеки scikit-learn:
- **Количество объектов**: 150
- **Количество признаков**: 4 (длина и ширина чашелистика и лепестка)
- **Количество классов**: 3 (setosa, versicolor, virginica)
- **Разделение**: 80% для обучения, 20% для тестирования

## Реализованные методы

### 1. AdaBoost (Adaptive Boosting)
Алгоритм адаптивного бустинга, который последовательно обучает слабые классификаторы (деревья решений), корректируя веса объектов на каждой итерации. Объекты, неправильно классифицированные на предыдущих итерациях, получают больший вес.

**Ключевые особенности реализации:**
- Использование деревьев решений как базовых классификаторов
- Адаптивное изменение весов объектов
- Вычисление коэффициента важности каждого классификатора (alpha)

### 2. Bagging (Bootstrap Aggregating)
Метод, основанный на обучении множества моделей на различных bootstrap-выборках из исходного датасета с последующим усреднением/голосованием их предсказаний.

**Ключевые особенности реализации:**
- Bootstrap-семплирование с возвращением
- Использование простого голосования для финального предсказания
- Параллельное обучение базовых моделей

### 3. Random Subspace Method (RSM)
Метод случайных подпространств, где каждый базовый классификатор обучается на случайном подмножестве признаков.

**Ключевые особенности реализации:**
- Случайный выбор подмножества признаков для каждой модели
- Использование голосования большинства для финального решения
- Снижение переобучения за счет уменьшения размерности

## Результаты экспериментов

### Сравнение точности и времени обучения

| Метод | Собственная реализация | Scikit-learn |
|-------|----------------------|--------------|
| **AdaBoost** | Accuracy: 1.0000, Time: 0.0225s | Accuracy: 1.0000, Time: 0.0086s |
| **Bagging** | Accuracy: 1.0000, Time: 0.0173s | Accuracy: 1.0000, Time: 0.0046s |
| **RSM** | Accuracy: 1.0000, Time: 0.0173s | Accuracy: 1.0000, Time: 0.0217s |


## Выводы

1. **Эффективность методов**: На датасете Iris методы показали отличные результаты, достигнув 100% точности. Это объясняется относительной простотой датасета и эффективностью ансамблевых методов.

2. **Качество реализации**: Собственные реализации работают на уровне библиотечных аналогов.

3. **Производительность**: Библиотечные реализации ожидаемо превосходят по скорости благодаря низкоуровневым оптимизациям, но разница не критична для данного размера датасета.

