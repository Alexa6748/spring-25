# Лабораторная работа №3  
## Наивный байесовский классификатор  
В рамках данной работы необходимо реализовать наивный байесовский классификатор, обучить его на датасете Glass Classification, оценить качество с помощью кросс-валидации, замерить время обучения и сравнить результаты с эталонной реализацией в библиотеке scikit-learn.

### Описание алгоритма
Наивный гауссовский классификатор — это алгоритм, основанный на теореме Байеса с предположением о нормальном (гауссовском) распределении признаков внутри каждого класса и условной независимости признаков. Алгоритм состоит из следующих шагов:

Априорные вероятности: вычисляем долю каждого класса в обучающем наборе.

Оценка параметров распределения: для каждого класса и каждой характеристики считаем среднее и дисперсию (с добавлением малой регуляризации для устойчивости).

Правдоподобие: на каждом новом объекте считаем плотность гауссовского распределения для каждого признака и перемножаем (или суммируем логарифмы) по всем признакам.

Постериорные вероятности: складываем логарифм априорной вероятности и логарифмы правдоподобий, выбираем класс с максимальным значением.

Ключевые особенности:

Условная независимость признаков упрощает вычисления.

Модель непрерывных признаков при помощи гауссовского распределения.

Широко применяется при высокой размерности благодаря своей простоте.

### Описание датасета  
Использовался набор Glass Classification из UCI ML Repository:

Размер: 214 объектов, 9 признаков.

Признаки:

RI (refractive index)

Na, Mg, Al, Si, K, Ca, Ba, Fe (содержание элементов)

Класс: Тип стекла (6 категорий: 0–5).

Метки были закодированы с помощью LabelEncoder.

### Эксперименты  
Эксперименты проводились с 5‑фолдовым кросс‑валидационным разбиением. Измерялись:

Accuracy

F1-macro

Время обучения (Train time)

Метод                     | Accuracy | F1-macro | Train time (s)
--------------------------|----------|----------|---------------
Hand‑NB (raw)             | 0.4743   | 0.4593   | 0.10          
sklearn NB (raw)          | 0.4696   | 0.4349   | 0.02          
Hand‑NB (scaled)          | 0.4743   | 0.4554   | 0.05          
sklearn NB (scaled)       | 0.4696   | 0.4349   | 0.02          

#### Интерпретация полученных результатов  
Сходство метрик: ручная реализация и GaussianNB из sklearn демонстрируют близкие значения Accuracy и F1, что подтверждает корректность алгоритма.

Время обучения: реализация sklearn работает примерно в 2–3 раза быстрее благодаря оптимизациям на C и векторизированным операциям.

Влияние стандартизации: масштабирование признаков (StandardScaler) не меняет общую точность, но делает разделяющие поверхности более симметричными и "эллипсоидными".

Границы решений: на паре признаков RI vs Na и Si vs Ca визуализация показывает сложные нелинейные области, объясняемые разными дисперсиями характеристик.

### Выводы  
Ручная реализация Naive Bayes корректна: метрики совпадают с эталоном.

Стандартизация признаков влияет на геометрию границ, но не на среднюю точность.

Эталонная реализация в sklearn работает существенно быстрее благодаря оптимизациям.

Наивный Байес устойчив к распределению признаков, но может давать сложные границы при сильно различающихся дисперсиях.
