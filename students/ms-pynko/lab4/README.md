# Лабораторная работа №4: Латентное размещение Дирихле (LDA)

Репозиторий содержит реализацию алгоритма Latent Dirichlet Allocation (LDA) методом Гиббсовского сэмплинга и сравнение с вариационной реализацией из библиотеки scikit-learn на корпусе 20 Newsgroups.

## Описание проекта

В работе реализован собственный алгоритм LDA через Гиббсовский сэмплинг и проведено сравнение с реализацией `LatentDirichletAllocation` из scikit-learn. Основная цель — оценить компромисс между качеством тематического моделирования (метрика UMass) и временем обучения.


## Описание алгоритма

1. Импортируются библиотеки и загружается корпус 20 Newsgroups (500 документов).
2. Реализуется собственный класс `GibbsLDA` для Гиббсовского сэмплинга.
3. Реализуется метрика UMass для оценки когерентности тем.
4. Обучаются две модели: собственная и `sklearn.LDA`.
5. Выводится таблица со временем обучения и средней когерентностью тем.

## Результаты

| Модель      | Время (с) | UMass   |
| ----------- | --------- | ------- |
| Custom LDA  | 48.79     | -1.1486 |
| sklearn LDA | 3.53      | -1.5407 |


## Выводы

* **Скорость:** `sklearn.LDA` быстрее за счёт вариационного вывода.
* **Качество:** кастомная реализация через Гиббсовский сэмплинг даёт более высокую когерентность тем, но требует больше времени.
* **Баланс:** в реальных задачах часто требуется найти компромисс между скоростью и качеством, настраивая гиперпараметры α и β.

