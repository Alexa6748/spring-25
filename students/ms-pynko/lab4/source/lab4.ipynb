{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab8fc1f-a033-4447-80d1-54919defdcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb2a5b6-30c6-4ad4-9e97-03bd3e4da38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    use_nltk = True\n",
    "except Exception:\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stopwords\n",
    "    PorterStemmer = None\n",
    "    use_nltk = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91330237-7b93-4660-b686-c468bb7dd4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
    "raw_docs = news.data[:500]\n",
    "\n",
    "if use_nltk and PorterStemmer:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "else:\n",
    "    stop_words = set(stopwords)\n",
    "    stemmer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583b606f-7472-490e-9808-1deb012d9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = re.sub(r'[^a-zA-Z]', ' ', text).lower().split()\n",
    "    res = []\n",
    "    for t in tokens:\n",
    "        if t not in stop_words and len(t)>2:\n",
    "            res.append(stemmer.stem(t) if stemmer else t)\n",
    "    return res\n",
    "docs = [preprocess(doc) for doc in raw_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36435394-a0e5-4f82-aed9-8b4e79e3646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GibbsLDA:\n",
    "    def __init__(self, K, alpha=0.1, beta=0.01, iterations=1000):\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iter = iterations\n",
    "\n",
    "    def fit(self, docs):\n",
    "        vocab = {w for doc in docs for w in doc}\n",
    "        self.word2id = {w:i for i,w in enumerate(vocab)}\n",
    "        V = len(vocab)\n",
    "        D = len(docs)\n",
    "        K = self.K\n",
    "\n",
    "        nw = np.zeros((K, V)) + self.beta      # слово-тема\n",
    "        nd = np.zeros((D, K)) + self.alpha     # док-тема\n",
    "        nwsum = np.zeros(K) + V*self.beta      # всего слов в темах\n",
    "        z = []  # темы для каждого слова\n",
    "\n",
    "        for d, doc in enumerate(docs):\n",
    "            z_doc = []\n",
    "            for w in doc:\n",
    "                wid = self.word2id[w]\n",
    "                topic = np.random.randint(K)\n",
    "                z_doc.append(topic)\n",
    "                nw[topic, wid] += 1\n",
    "                nd[d, topic] += 1\n",
    "                nwsum[topic] += 1\n",
    "            z.append(z_doc)\n",
    "\n",
    "        for it in range(self.iter):\n",
    "            for d, doc in enumerate(docs):\n",
    "                for i, w in enumerate(doc):\n",
    "                    wid = self.word2id[w]\n",
    "                    t = z[d][i]\n",
    "                    # удаляем\n",
    "                    nw[t, wid] -= 1\n",
    "                    nd[d, t] -= 1\n",
    "                    nwsum[t] -= 1\n",
    "                    # вероятности\n",
    "                    p = (nd[d] * nw[:, wid] / nwsum)\n",
    "                    p /= p.sum()\n",
    "                    # новый топик\n",
    "                    new_t = np.random.choice(K, p=p)\n",
    "                    z[d][i] = new_t\n",
    "                    nw[new_t, wid] += 1\n",
    "                    nd[d, new_t] += 1\n",
    "                    nwsum[new_t] += 1\n",
    "\n",
    "        self.phi = nw / nwsum[:, None]\n",
    "        self.theta = nd / nd.sum(axis=1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae963273-9290-4271-9bde-f6703af9c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_umass(phi, word2id, docs, top_n=10):\n",
    "    id2w = {i:w for w,i in word2id.items()}\n",
    "    D_w = defaultdict(int)\n",
    "    D_pair = defaultdict(int)\n",
    "    for doc in docs:\n",
    "        uniq = set(doc)\n",
    "        for w in uniq:\n",
    "            D_w[w] += 1\n",
    "        for w1 in uniq:\n",
    "            for w2 in uniq:\n",
    "                if w1 < w2:\n",
    "                    D_pair[(w1,w2)] +=1\n",
    "    scores = []\n",
    "    K = phi.shape[0]\n",
    "    for k in range(K):\n",
    "        top_ids = np.argsort(phi[k])[-top_n:]\n",
    "        words = [id2w[i] for i in top_ids]\n",
    "        sc, cnt = 0,0\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i):\n",
    "                w1,w2 = words[i],words[j]\n",
    "                pair = tuple(sorted((w1,w2)))\n",
    "                Dp = D_pair.get(pair,0)\n",
    "                if D_w[w2]>0:\n",
    "                    sc += np.log((Dp+1)/D_w[w2])\n",
    "                    cnt +=1\n",
    "        scores.append(sc/cnt if cnt else 0)\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1710ce06-10cb-4855-a1f8-baffa00ad658",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10; iters=50\n",
    "\n",
    "lda_custom = GibbsLDA(K, iterations=iters)\n",
    "start = time.time()\n",
    "lda_custom.fit(docs)\n",
    "time_custom = time.time()-start\n",
    "umass_custom = compute_umass(lda_custom.phi, lda_custom.word2id, docs)\n",
    "\n",
    "vec = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X = vec.fit_transform([' '.join(d) for d in docs])\n",
    "lda_sk = LatentDirichletAllocation(n_components=K, learning_method='online',\n",
    "                                   random_state=42, doc_topic_prior=0.1,\n",
    "                                   topic_word_prior=0.01, max_iter=iters)\n",
    "start = time.time()\n",
    "lda_sk.fit(X)\n",
    "time_sk = time.time()-start\n",
    "class Wrap:\n",
    "    def __init__(self,m,fn):\n",
    "        self.phi = m.components_ / m.components_.sum(axis=1)[:,None]\n",
    "        self.word2id = {w:i for i,w in enumerate(fn)}\n",
    "lda_wr = Wrap(lda_sk, vec.get_feature_names_out())\n",
    "umass_sk = compute_umass(lda_wr.phi, lda_wr.word2id, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae60fd1d-4ddb-49ed-a76a-a6b971206d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель          | Время (с)  | UMass   \n",
      "----------------------------------------\n",
      "Custom LDA      | 48.79      | -1.1486 \n",
      "sklearn LDA     | 3.53       | -1.5407 \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Модель':<15} | {'Время (с)':<10} | {'UMass':<8}\")\n",
    "print('-'*40)\n",
    "print(f\"{'Custom LDA':<15} | {time_custom:<10.2f} | {umass_custom:<8.4f}\")\n",
    "print(f\"{'sklearn LDA':<15} | {time_sk:<10.2f} | {umass_sk:<8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61245d6-3630-4a6a-a20b-7a2e2742c35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
