{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8bd06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841a2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_sklearn(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for comp in model.components_:\n",
    "        top_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "        topics.append([feature_names[i] for i in top_idx])\n",
    "    return topics\n",
    "\n",
    "\n",
    "def compute_coherence(topics, corpus_bow, dictionary, texts):\n",
    "    cm_umass = CoherenceModel(\n",
    "        topics=topics, corpus=corpus_bow, dictionary=dictionary, coherence=\"u_mass\"\n",
    "    )\n",
    "    cm_cv = CoherenceModel(\n",
    "        topics=topics, texts=texts, dictionary=dictionary, coherence=\"c_v\"\n",
    "    )\n",
    "    return cm_umass.get_coherence(), cm_cv.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_EM:\n",
    "    def __init__(self, n_topics, n_iter=30, alpha=0.1, beta=0.01, random_state=42):\n",
    "        self.K = n_topics\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = csr_matrix(X, dtype=np.float64)\n",
    "        D, V = X.shape\n",
    "\n",
    "        self.phi = np.random.dirichlet(np.ones(V), self.K)\n",
    "        self.theta = np.random.dirichlet(np.ones(self.K), D)\n",
    "\n",
    "        for _ in tqdm(range(self.n_iter), desc=\"EM iterations\"):\n",
    "            n_wt = np.zeros((self.K, V))\n",
    "            n_td = np.zeros((D, self.K))\n",
    "\n",
    "            rows, cols = X.nonzero()\n",
    "            data = X.data\n",
    "            for idx, cnt in enumerate(data):\n",
    "                d, w = rows[idx], cols[idx]\n",
    "                p = self.phi[:, w] * self.theta[d]\n",
    "                p /= p.sum()\n",
    "                n_wt[:, w] += cnt * p\n",
    "                n_td[d] += cnt * p\n",
    "\n",
    "            self.phi = n_wt + self.beta\n",
    "            self.phi /= self.phi.sum(axis=1, keepdims=True)\n",
    "\n",
    "            self.theta = n_td + self.alpha\n",
    "            self.theta /= self.theta.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, n_iter=20):\n",
    "        X = csr_matrix(X, dtype=np.float64)\n",
    "        D, _ = X.shape\n",
    "        theta_new = np.random.dirichlet(np.ones(self.K), D)\n",
    "\n",
    "        for _ in range(n_iter):\n",
    "            n_td = np.zeros_like(theta_new)\n",
    "            rows, cols = X.nonzero()\n",
    "            data = X.data\n",
    "            for idx, cnt in enumerate(data):\n",
    "                d, w = rows[idx], cols[idx]\n",
    "                p = self.phi[:, w] * theta_new[d]\n",
    "                p /= p.sum()\n",
    "                n_td[d] += cnt * p\n",
    "\n",
    "            theta_new = n_td + self.alpha\n",
    "            theta_new /= theta_new.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return theta_new\n",
    "\n",
    "    def get_topics(self, feature_names, n_top_words=10):\n",
    "        topics = []\n",
    "        for k in range(self.K):\n",
    "            top_idx = np.argsort(self.phi[k])[::-1][:n_top_words]\n",
    "            topics.append([feature_names[i] for i in top_idx])\n",
    "        return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9470308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\")\n",
    ").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.lower().split() for doc in data]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus_bow = [dictionary.doc2bow(txt) for txt in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    min_df=5, max_df=0.5, stop_words=\"english\", max_features=2000\n",
    ")\n",
    "X = vectorizer.fit_transform(data)\n",
    "feat_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ef7e7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EM iterations: 100%|██████████| 50/50 [02:54<00:00,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 174.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics (EM): [['good', 'time', 'year', 'better', 'like', 'just', 'years', 'db', 'best', 'make'], ['drive', 'windows', 'thanks', 'use', 'card', 'does', 'problem', 'dos', 'disk', 'scsi'], ['10', '00', '20', '15', '25', '12', '14', '50', '16', '11'], ['game', 'team', 'games', 'play', 'season', 'hockey', 'league', 'players', 'cx', 'period'], ['key', 'use', 'government', 'law', 'gun', 'public', 'encryption', 'used', 'chip', 'people'], ['god', 'people', 'does', 'jesus', 'believe', 'say', 'true', 'think', 'life', 'bible'], ['people', 'armenian', 'israel', 'state', 'states', 'said', 'armenians', 'war', 'new', 'turkish'], ['edu', 'com', 'file', 'space', 'available', 'information', 'program', 'mail', 'use', 'ftp'], ['ax', 'max', 'g9v', 'b8f', 'a86', 'pl', '145', '1d9', '0t', '1t'], ['don', 'know', 'just', 'think', 'like', 'people', 'going', 've', 'want', 'did']]\n",
      "Coherence UMass: -4.2270, C_V: 0.5170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "lda_em = LDA_EM(n_topics=10, n_iter=50, alpha=0.1, beta=0.01)\n",
    "lda_em.fit(X)\n",
    "print(f\"Time: {time() - start:.2f}s\")\n",
    "\n",
    "topics_em = lda_em.get_topics(feat_names, n_top_words=10)\n",
    "umass_em, cv_em = compute_coherence(topics_em, corpus_bow, dictionary, texts)\n",
    "print(\"Topics (EM):\", topics_em)\n",
    "print(f\"Coherence UMass: {umass_em:.4f}, C_V: {cv_em:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.23s\n",
      "Topics (sklearn): [['space', 'use', 'men', 'university', 'people', 'ago', 'class', 'station', 'nasa', 'control'], ['mr', 'president', 'think', 'going', 'know', 'don', 'people', 'jobs', 'time', 'just'], ['space', 'nasa', 'orbit', 'launch', 'lunar', 'earth', 'shuttle', 'surface', '93', 'satellite'], ['just', 'car', 'like', 'think', 'know', 'people', 'don', 'problem', 've', 'use'], ['edu', 'graphics', 'data', 'software', 'image', 'ftp', 'available', 'information', 'package', 'code']]\n",
      "Coherence UMass: -4.3693, C_V: 0.4994\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "lda_sk = LatentDirichletAllocation(n_components=10, max_iter=50, random_state=42)\n",
    "lda_sk.fit(X)\n",
    "print(f\"Time: {time() - start:.2f}s\")\n",
    "\n",
    "topics_sk = get_topics_sklearn(lda_sk, feat_names, n_top_words=10)\n",
    "umass_sk, cv_sk = compute_coherence(topics_sk, corpus_bow, dictionary, texts)\n",
    "print(\"Topics (sklearn):\", topics_sk)\n",
    "print(f\"Coherence UMass: {umass_sk:.4f}, C_V: {cv_sk:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
