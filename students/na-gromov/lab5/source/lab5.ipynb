{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import NMF\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationRecommender:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_factors=50,\n",
    "        learning_rate=0.005,\n",
    "        regularization=0.02,\n",
    "        n_epochs=10,\n",
    "        min_learning_rate=0.0001,\n",
    "        patience=5,\n",
    "        random_state=42,\n",
    "    ):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.patience = patience\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.user_bias = None\n",
    "        self.item_bias = None\n",
    "        self.global_bias = None\n",
    "        self.n_users = None\n",
    "        self.n_items = None\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def _initialize_parameters(self, n_users, n_items):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        scale = np.sqrt(2.0 / (n_users + self.n_factors))\n",
    "        self.user_factors = np.random.normal(0, scale, (n_users, self.n_factors))\n",
    "        self.item_factors = np.random.normal(0, scale, (n_items, self.n_factors))\n",
    "        self.user_bias = np.zeros(n_users)\n",
    "        self.item_bias = np.zeros(n_items)\n",
    "        self.global_bias = 0.0\n",
    "\n",
    "    def _validate_indices(self, user_idx, item_idx):\n",
    "        user_idx = int(user_idx)\n",
    "        item_idx = int(item_idx)\n",
    "\n",
    "        if user_idx < 0 or user_idx >= self.n_users:\n",
    "            raise ValueError(\n",
    "                f\"User index {user_idx} out of bounds [0, {self.n_users - 1}]\"\n",
    "            )\n",
    "        if item_idx < 0 or item_idx >= self.n_items:\n",
    "            raise ValueError(\n",
    "                f\"Item index {item_idx} out of bounds [0, {self.n_items - 1}]\"\n",
    "            )\n",
    "\n",
    "        return user_idx, item_idx\n",
    "\n",
    "    def _predict_single(self, user_idx, item_idx):\n",
    "        user_idx, item_idx = self._validate_indices(user_idx, item_idx)\n",
    "        return (\n",
    "            self.global_bias\n",
    "            + self.user_bias[user_idx]\n",
    "            + self.item_bias[item_idx]\n",
    "            + np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "        )\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        for u, i, r in data:\n",
    "\n",
    "            pred = self._predict_single(int(u), int(i))\n",
    "            predictions.append(pred)\n",
    "            actuals.append(r)\n",
    "\n",
    "        if not predictions:\n",
    "            return float(\"inf\")\n",
    "        return np.sqrt(mean_squared_error(actuals, predictions))\n",
    "\n",
    "    def _adaptive_learning_rate(self, epoch):\n",
    "        return max(self.learning_rate * (0.9**epoch), self.min_learning_rate)\n",
    "\n",
    "    def fit(self, train_data, val_data=None, verbose=True):\n",
    "        n_users = int(np.max(train_data[:, 0])) + 1\n",
    "        n_items = int(np.max(train_data[:, 1])) + 1\n",
    "\n",
    "        if val_data is not None:\n",
    "            n_users = max(n_users, int(np.max(val_data[:, 0])) + 1)\n",
    "            n_items = max(n_items, int(np.max(val_data[:, 1])) + 1)\n",
    "\n",
    "        self._initialize_parameters(n_users, n_items)\n",
    "        self.global_bias = np.mean(train_data[:, 2])\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(train_data)\n",
    "\n",
    "            current_lr = self._adaptive_learning_rate(epoch)\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            valid_updates = 0\n",
    "\n",
    "            for user_idx, item_idx, rating in train_data:\n",
    "\n",
    "                user_idx, item_idx = self._validate_indices(\n",
    "                    int(user_idx), int(item_idx)\n",
    "                )\n",
    "\n",
    "                prediction = self._predict_single(user_idx, item_idx)\n",
    "                error = rating - prediction\n",
    "                epoch_loss += error**2\n",
    "                valid_updates += 1\n",
    "\n",
    "                self.user_bias[user_idx] += current_lr * (\n",
    "                    error - self.regularization * self.user_bias[user_idx]\n",
    "                )\n",
    "                self.item_bias[item_idx] += current_lr * (\n",
    "                    error - self.regularization * self.item_bias[item_idx]\n",
    "                )\n",
    "\n",
    "                user_factor = self.user_factors[user_idx].copy()\n",
    "                item_factor = self.item_factors[item_idx].copy()\n",
    "\n",
    "                self.user_factors[user_idx] += current_lr * (\n",
    "                    error * item_factor - self.regularization * user_factor\n",
    "                )\n",
    "                self.item_factors[item_idx] += current_lr * (\n",
    "                    error * user_factor - self.regularization * item_factor\n",
    "                )\n",
    "\n",
    "            train_loss = np.sqrt(epoch_loss / valid_updates)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            if val_data is not None:\n",
    "                val_loss = self._compute_loss(val_data)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch + 1}/{self.n_epochs} - \"\n",
    "                        f\"Train RMSE: {train_loss:.4f} - \"\n",
    "                        f\"Val RMSE: {val_loss:.4f}\"\n",
    "                    )\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                        break\n",
    "            elif verbose:\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1}/{self.n_epochs} - Train RMSE: {train_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_idx, item_idx):\n",
    "        return self._predict_single(user_idx, item_idx)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "\n",
    "        for u, i, r in test_data:\n",
    "\n",
    "            pred = self._predict_single(int(u), int(i))\n",
    "            predictions.append(pred)\n",
    "            actuals.append(r)\n",
    "\n",
    "        if not predictions:\n",
    "            return {\"rmse\": float(\"inf\"), \"mae\": float(\"inf\")}\n",
    "\n",
    "        return {\n",
    "            \"rmse\": np.sqrt(mean_squared_error(actuals, predictions)),\n",
    "            \"mae\": mean_absolute_error(actuals, predictions),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 943\n",
      "Number of items: 1682\n",
      "Number of ratings: 100000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../../ml-100k/u.data\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"],\n",
    ")\n",
    "\n",
    "print(f\"Number of users: {data['user_id'].nunique()}\")\n",
    "print(f\"Number of items: {data['item_id'].nunique()}\")\n",
    "print(f\"Number of ratings: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = sorted(data[\"user_id\"].unique())\n",
    "item_ids = sorted(data[\"item_id\"].unique())\n",
    "\n",
    "user_id_map = {old_id: new_id for new_id, old_id in enumerate(user_ids)}\n",
    "item_id_map = {old_id: new_id for new_id, old_id in enumerate(item_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"user_id\"] = data[\"user_id\"].map(user_id_map)\n",
    "data[\"item_id\"] = data[\"item_id\"].map(item_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = sklearn_train_test_split(\n",
    "    data[[\"user_id\", \"item_id\", \"rating\"]].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "val_data, test_data = sklearn_train_test_split(\n",
    "    test_data, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 80000 samples\n",
      "Validation set: 10000 samples\n",
      "Test set: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set: {len(train_data)} samples\")\n",
    "print(f\"Validation set: {len(val_data)} samples\")\n",
    "print(f\"Test set: {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train RMSE: 1.0480 - Val RMSE: 1.0075\n",
      "Epoch 2/10 - Train RMSE: 0.9840 - Val RMSE: 0.9818\n",
      "Epoch 3/10 - Train RMSE: 0.9631 - Val RMSE: 0.9707\n",
      "Epoch 4/10 - Train RMSE: 0.9518 - Val RMSE: 0.9644\n",
      "Epoch 5/10 - Train RMSE: 0.9445 - Val RMSE: 0.9602\n",
      "Epoch 6/10 - Train RMSE: 0.9394 - Val RMSE: 0.9576\n",
      "Epoch 7/10 - Train RMSE: 0.9354 - Val RMSE: 0.9556\n",
      "Epoch 8/10 - Train RMSE: 0.9323 - Val RMSE: 0.9541\n",
      "Epoch 9/10 - Train RMSE: 0.9297 - Val RMSE: 0.9530\n",
      "Epoch 10/10 - Train RMSE: 0.9276 - Val RMSE: 0.9520\n",
      "Training time: 15.04 seconds\n",
      "Test RMSE: 0.9430\n",
      "Test MAE: 0.7466\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = MatrixFactorizationRecommender(\n",
    "    n_factors=50,\n",
    "    learning_rate=0.005,\n",
    "    regularization=0.02,\n",
    "    n_epochs=10,\n",
    "    patience=5,\n",
    ")\n",
    "\n",
    "model.fit(train_data, val_data, verbose=True)\n",
    "custom_time = time.time() - start_time\n",
    "\n",
    "custom_metrics = model.evaluate(test_data)\n",
    "\n",
    "print(f\"Training time: {custom_time:.2f} seconds\")\n",
    "print(f\"Test RMSE: {custom_metrics['rmse']:.4f}\")\n",
    "print(f\"Test MAE: {custom_metrics['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.15 seconds\n",
      "Test RMSE: 2.6800\n",
      "Test MAE: 2.4184\n"
     ]
    }
   ],
   "source": [
    "n_users = len(user_id_map)\n",
    "n_items = len(item_id_map)\n",
    "R = np.zeros((n_users, n_items))\n",
    "for user, item, rating in train_data:\n",
    "    R[int(user), int(item)] = rating\n",
    "\n",
    "start_time = time.time()\n",
    "sklearn_model = NMF(\n",
    "    n_components=50,\n",
    "    init=\"random\",\n",
    "    random_state=42,\n",
    "    max_iter=10,\n",
    ")\n",
    "\n",
    "sklearn_model.fit(R)\n",
    "sklearn_time = time.time() - start_time\n",
    "\n",
    "sklearn_preds = sklearn_model.transform(R) @ sklearn_model.components_\n",
    "test_preds = []\n",
    "test_actuals = []\n",
    "for user, item, rating in test_data:\n",
    "    test_preds.append(sklearn_preds[int(user), int(item)])\n",
    "    test_actuals.append(rating)\n",
    "\n",
    "sklearn_rmse = np.sqrt(mean_squared_error(test_actuals, test_preds))\n",
    "sklearn_mae = mean_absolute_error(test_actuals, test_preds)\n",
    "\n",
    "print(f\"Training time: {sklearn_time:.2f} seconds\")\n",
    "print(f\"Test RMSE: {sklearn_rmse:.4f}\")\n",
    "print(f\"Test MAE: {sklearn_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.33 seconds\n",
      "Test RMSE: 0.9465\n",
      "Test MAE: 0.7500\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "surprise_data = Dataset.load_from_df(data[[\"user_id\", \"item_id\", \"rating\"]], reader)\n",
    "\n",
    "trainset, testset = surprise_train_test_split(\n",
    "    surprise_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "surprise_model = SVD(\n",
    "    n_factors=50, n_epochs=10, lr_all=0.005, reg_all=0.02, random_state=42\n",
    ")\n",
    "\n",
    "surprise_model.fit(trainset)\n",
    "surprise_time = time.time() - start_time\n",
    "\n",
    "surprise_predictions = surprise_model.test(testset)\n",
    "surprise_rmse = accuracy.rmse(surprise_predictions, verbose=False)\n",
    "surprise_mae = accuracy.mae(surprise_predictions, verbose=False)\n",
    "\n",
    "print(f\"Training time: {surprise_time:.2f} seconds\")\n",
    "print(f\"Test RMSE: {surprise_rmse:.4f}\")\n",
    "print(f\"Test MAE: {surprise_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric          Custom Model    Scikit-learn    Surprise       \n",
      "------------------------------------------------------------\n",
      "Training Time   15.04s          0.15s           0.33s\n",
      "Test RMSE       0.9430          2.6800          0.9465\n",
      "Test MAE        0.7466          2.4184          0.7500\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Metric':<15} {'Custom Model':<15} {'Scikit-learn':<15} {'Surprise':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(\n",
    "    f\"{'Training Time':<15} {custom_time:.2f}s{'':<9} {sklearn_time:.2f}s{'':<10} {surprise_time:.2f}s\"\n",
    ")\n",
    "print(\n",
    "    f\"{'Test RMSE':<15} {custom_metrics['rmse']:.4f}{'':<9} {sklearn_rmse:.4f}{'':<9} {surprise_rmse:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"{'Test MAE':<15} {custom_metrics['mae']:.4f}{'':<9} {sklearn_mae:.4f}{'':<9} {surprise_mae:.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
