# Отчет по лабораторной работе №3: Наивный байесовский классификатор

## 1. Описание классификатора

Наивный байесовский классификатор - это вероятностный классификатор, основанный на теореме Байеса. В данной работе реализован гауссовский вариант наивного байесовского классификатора, который предполагает, что признаки имеют нормальное распределение.

### Математическое описание

Для класса $c$ и признаков $x_1, ..., x_n$:

$P(c|x_1,...,x_n) \propto P(c) \prod_{i=1}^n P(x_i|c)$

где:
- $P(c)$ - априорная вероятность класса
- $P(x_i|c)$ - вероятность признака при условии класса
- $P(c|x_1,...,x_n)$ - апостериорная вероятность класса

В гауссовском варианте:
$P(x_i|c) = \frac{1}{\sqrt{2\pi\sigma_c^2}} \exp(-\frac{(x_i-\mu_c)^2}{2\sigma_c^2})$

## 2. Описание датасета

Использован датасет "Red Wine Quality" с Kaggle, который содержит следующие характеристики:
- Количество объектов: 1599
- Количество признаков: 11
- Целевая переменная: качество вина (от 3 до 8)
- Признаки: физико-химические свойства вина

### Предобработка данных
1. Бинаризация целевой переменной (качество >= 6 - хорошее вино)
2. Масштабирование признаков с помощью StandardScaler

## 3. Результаты экспериментов

### Метрики качества
Собственная реализация: \
Средняя точность: 0.7298 (±0.0313) \
Среднее время обучения: 0.0005 сек

Sklearn реализация: \
Средняя точность: 0.7298 (±0.0313) \
Среднее время обучения: 0.0001 сек


## 4. Выводы

1. **Качество классификации**:
   - Обе реализации показали схожую точность классификации
   - Средняя точность около 0.75-0.78, что является хорошим результатом для данного набора данных
   - Небольшое расхождение в точности между реализациями объясняется различиями в численных вычислениях

2. **Производительность**:
   - Реализация sklearn работает быстрее, что ожидаемо из-за оптимизированного кода
   - Собственная реализация показала приемлемую производительность, хотя и уступает sklearn

