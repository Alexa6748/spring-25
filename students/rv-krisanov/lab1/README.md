# Отчет по лабораторной работе №1: Метод случайных подпространств (RSM)

## Введение

В данной лабораторной работе была реализована одна из техник ансамблевого обучения - метод случайных подпространств (Random Subspace Method, RSM). Этот метод представляет собой эффективный подход к созданию ансамблей моделей путем обучения базовых алгоритмов на различных подмножествах признаков.

## Описание метода случайных подпространств (RSM)

Метод случайных подпространств (RSM) - это ансамблевый метод машинного обучения, предложенный Tin Kam Ho в 1998 году. Основная идея метода заключается в следующем:

1. Для каждого базового алгоритма случайным образом выбирается подмножество признаков из исходного набора данных (подпространство)
2. Базовые алгоритмы обучаются на полной выборке объектов, но только на выбранном подмножестве признаков
3. При предсказании результаты всех базовых алгоритмов агрегируются (например, путем голосования в задачах классификации)

Ключевые особенности метода:
- В отличие от бэггинга, RSM использует все объекты выборки, но варьирует набор признаков
- Метод особенно эффективен при работе с данными, имеющими большое количество признаков
- Способствует уменьшению корреляции между базовыми моделями, что повышает обобщающую способность ансамбля
- Может помочь в борьбе с проклятием размерности

## Описание датасета

В работе использовался датасет Rice Dataset, содержащий информацию о двух сортах риса: Gonen и Jasmine. Датасет содержит морфологические признаки зерен риса.

## Реализация метода

Была создана собственная реализация метода случайных подпространств с использованием деревьев решений в качестве базовых алгоритмов:

```python
class RandomSubspaceMethod:
    def __init__(
        self,
        BaseEstimator=DecisionTreeClassifier,
        n_estimators=10,
        subspace_size=0.5,
        random_state=None,
    ):
        self.BaseEstimator = BaseEstimator
        self.n_estimators = n_estimators
        self.subspace_size = subspace_size
        self.random_state = random_state
        self.estimators = []
        self.feature_indices = []

    def fit(self, X: np.ndarray | pd.DataFrame, y: np.ndarray):
        _n_samples, n_features = X.shape
        n_subspace_features = max(1, int(n_features * self.subspace_size))

        rng = np.random.RandomState(self.random_state)

        for i in range(self.n_estimators):
            feature_indices = rng.choice(n_features, n_subspace_features, replace=False)
            self.feature_indices.append(feature_indices)

            estimator = self.BaseEstimator()
            estimator.fit(X[:, feature_indices], y)
            self.estimators.append(estimator)

        return self

    def predict(self, X):
        predictions = np.zeros((X.shape[0], len(self.estimators)))

        for estimator_idx, (estimator, features) in enumerate(
            zip(self.estimators, self.feature_indices)
        ):
            predictions[:, estimator_idx] = estimator.predict(X[:, features])

        return np.array([np.bincount(row.astype(int)).argmax() for row in predictions])
```

Основные параметры:
- `BaseEstimator` - базовый алгоритм (по умолчанию - DecisionTreeClassifier)
- `n_estimators` - количество базовых алгоритмов в ансамбле
- `subspace_size` - доля признаков, используемых каждым базовым алгоритмом
- `random_state` - параметр для воспроизводимости результатов

## Результаты экспериментов

Для оценки качества модели была использована 5-fold кросс-валидация с метрикой F1-score.

### Результаты собственной реализации RSM:

```
{'scores': [0.9834254143646409, 1.0, 1.0, 1.0, 1.0],
 'mean_score': 0.9966850828729281}
```

### Результаты эталонной реализации (scikit-learn):

```
{'scores': [0.994413407821229,
  1.0,
  0.9704142011834319,
  1.0,
  0.9885057471264368],
 'mean_score': 0.9906666712262195}
```

### Время обучения:

- **Собственная реализация**: 10.8 ms ± 95.1 μs
- **Реализация scikit-learn**: 16.2 ms ± 71 μs

## Сравнение с эталонной реализацией

Для сравнения с нашей реализацией мы использовали `BaggingClassifier` из библиотеки scikit-learn со следующими параметрами:

```python
rsm_sklearn = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=10,
    max_features=0.5,
    bootstrap=False,
    bootstrap_features=True,
    random_state=0,
)
```

Сравнение результатов:

| Параметр | Собственная реализация | Реализация scikit-learn |
|----------|--------------------------|--------------------------|
| Средний F1-score | 0.9967 | 0.9907 |
| Время обучения | 10.8 ms | 16.2 ms |

Из результатов видно, что:
1. Собственная реализация показывает немного более высокий средний F1-score (0.9967 против 0.9907)
2. Собственная реализация работает быстрее (10.8 ms против 16.2 ms)

## Выводы

1. **Эффективность метода RSM**: Метод случайных подпространств показал высокую эффективность на данном датасете, достигнув F1-score близкого к 1.0. Это свидетельствует о хорошей обобщающей способности данного ансамблевого метода.

2. **Сравнение с эталонной реализацией**: Наша реализация показала сопоставимые (и даже немного лучшие) результаты по сравнению с библиотечной реализацией, как с точки зрения точности предсказаний, так и с точки зрения скорости обучения.

3. **Применимость метода**: Метод RSM особенно полезен для задач с относительно небольшим количеством объектов и большим количеством признаков, так как он позволяет эффективно использовать всю выборку, варьируя только набор используемых признаков.

4. **Простота реализации**: Реализация метода RSM достаточно проста и понятна, что делает его доступным для использования в различных задачах машинного обучения.

В целом, метод случайных подпространств представляет собой мощный инструмент в арсенале методов ансамблевого обучения, особенно когда речь идет о задачах с большим числом признаков.
