# Лабораторная работа №2. Градиентный бустинг

## Описание алгоритма
Градиентный бустинг — это ансамблевый метод, который последовательно строит слабые модели (обычно деревья решений), где каждая последующая модель корректирует ошибки предыдущих. 

**Математическая основа для регрессии:**

На каждой итерации `m`:
   - Вычисление остатков: 
   
     $$r_i = y_i - F_{m-1}(x_i)$$
     
   - Обучение дерева $$h_m$$ на остатках
   - Обновление предсказаний: 

     $$F_m(x) = F_{m-1}(x) + η·h_m(x)$$

   где $$η$$ — скорость обучения.

## Датасет
**California Housing Dataset** (встроенный в sklearn):
- Образцов: 20,640
- Признаков: 8 (координаты дома, средний возраст жильцов и т.д.)
- Целевая переменная: Средняя стоимость дома (в сотнях тысяч $)

**Предобработка:**
- Нормализация признаков с помощью `MinMaxScaler`

## Реализация
[Код](source/gradboosting.py)

## Эксперименты

### Гиперпараметры моделей
| Параметр              | Sklearn          | Custom           |
|-----------------------|------------------|------------------|
| n_estimators          | 200              | 200              |
| learning_rate         | 0.1              | 0.1              |
| max_depth             | 5                | 5                |
| subsample             | 1.0              | 0.8              |

### Результаты качества (R²)
| Метрика               | Sklearn          | Custom           |
|-----------------------|------------------|------------------|
| R² на тестовой выборке| 0.8306           | 0.8300           |

### Время обучения (n_estimators=100)
| Модель                | Время (mean ± std) |
|-----------------------|--------------------|
| Sklearn               | 8.24 s ± 334 ms   |
| Custom                | 7.82 s ± 488 ms   |

## Сравнение с эталонной реализацией
1. **Качество моделей**:
   - Показатели R² практически идентичны (разница 0.0006)
   - Небольшое отставание кастомной реализации объясняется:
     - Упрощенной логикой расчета градиентов
     - Отсутствием регуляризации

2. **Производительность**:
   - Кастомная реализация немного быстрее (7.82s vs 8.24s)
   - Причины:
     - Использование субсемплирования (subsample=0.8)

## Выводы
1. **Успешность реализации**:
   - Кастомная модель достигла качества, сравнимого с sklearn (R² > 0.82)
   - Основные идеи градиентного бустинга реализованы

2. **Оптимизации**:
   - Субсемплирование дает выигрыш в скорости без потери качества
   - Глубина деревьев (max_depth=8) оказалась оптимальной
