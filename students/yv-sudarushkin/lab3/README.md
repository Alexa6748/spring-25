# Лабораторная работа №3. Наивный байесовский классификатор

## Описание наивного байесовского классификатора
Наивный байесовский классификатор — вероятностный алгоритм, основанный на **теореме Байеса** и предположении о **независимости признаков**. Для непрерывных данных используется **гауссовская модель**, где правдоподобие признака вычисляется по формуле:  
$$P(x_i|y)=\frac{1}{\sqrt{2\pi\sigma_y^2}}\exp\left(-\frac{(x_i-\mu_y)^2}{2\sigma_y^2}\right)$$ 

где:
- μ<sub>y</sub> — среднее значение признака для класса y,
- σ<sub>y</sub><sup>2</sup> — дисперсия признака для класса y.

## Описание датасета
**Датасет:** Rice (Gonen and Jasmine)  
**Источник:** [Kaggle](https://www.kaggle.com/datasets/seymasa/rice-dataset-gonenjasmine).  
**Цель:** Классификация рисовых зерен на два сорта: **Gonen** (0) и **Jasmine** (1).  
**Признаки:**
- Числовые характеристики зерен: `Area`, `Perimeter`, `Major_Axis_Length` и др.  
**Объем данных:** 
- 18185 образцов.

## Результаты экспериментов
### Точность моделей
| Метрика                         | Custom Model    | Scikit-learn    |
|---------------------------------|-----------------|-----------------|
| Точность (тест)                 | 0.9837          | 0.9837          |
| Средняя точность (CV, 5 фолдов) | 0.9837 ± 0.0022 | 0.9837 ± 0.0022 |

### Время обучения
| Модель       | Время (mean ± std. dev.) |
|--------------|--------------------------|
| Custom       | 3.9 ms ± 602 μs          |
| Scikit-learn | 7.28 ms ± 2.15 ms        |

## Сравнение с эталонной реализацией
1. **Точность:** Обе модели показали **идентичную точность** как на тестовой выборке, так и при кросс-валидации. Это подтверждает корректность реализации классификатора.  
2. **Время обучения:** Кастомная модель обучается **быстрее** (3.9 мс против 7.28 мс). Возможные причины:
   - Оптимизация вычислений вручную (например, отказ от избыточных проверок).
   - Особенности реализации матричных операций в NumPy.

## Выводы
1. Кастомная реализация **не уступает в точности** эталонной, но превосходит ее в скорости обучения.  
2. Результаты демонстрируют, что для малых и средних датасетов ручная реализация может быть оптимальным решением.